{
  "hash": "368e40011dc28a4af7d9a8885bb2259b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Application of Artificial Intelligence on the Centrifugal Pump Operation\"\nauthor: \"Mohammed Twheed Khater\"\ndate: \"2024-06-24\"\ncache: false\nlisting:\n  - id: gallery\n    template: gallery.ejs\n    contents: gallery.yml\n---\n\n# Chapter 1: Prediction of the Effect of Impeller Trimming on Centrifugal Pump Performance Using AI\n\n## Introduction\n\nCentrifugal pumps are pivotal components in various industrial applications, from water supply systems to chemical processing. Optimizing their performance is crucial for enhancing energy efficiency and reducing operational costs. One key optimization technique is impeller trimming, which involves reducing the diameter of a pump impeller to align the pump's performance more closely with the system requirements. This chapter explores the concept of impeller trimming, its significance, traditional methods, and the advantages of employing Artificial Intelligence (AI) for performance prediction.\n\n## The Concept of Impeller Trimming\n\n### What is Trimming?\n\nImpeller trimming is the process of mechanically reducing the diameter of the pump impeller. This adjustment directly influences the pumpâ€™s head and flow rate, thereby modifying its performance characteristics. Trimming is performed to ensure that the pump operates within the desired performance range, avoiding over-delivery or under-delivery of fluid.\n\n### Why Trimming?\n\n### The Need for Trimming\n\nTrimming the impeller allows for the customization of pump performance to meet specific operational requirements. This customization is particularly necessary when:\n- The available pump sizes do not perfectly match the required system specifications.\n- System demands change over time, necessitating adjustments to maintain optimal efficiency.\n- Reducing the operational costs by minimizing energy wastage.\n\n\n#### Energy Consumption\n\nCentrifugal pumps are often responsible for a significant portion of the energy consumption in industrial settings. Trimming the impeller to match the exact system requirements can greatly reduce the energy consumption of the pump. By operating more efficiently, the pump uses less power, leading to substantial energy savings *where for each 1 kw in pump delivery corresponds to 6 kwhr in power station*.\n\n#### Market Availability\n\nThe pumps available in the market may not always fit specific system requirements precisely. Typically, pumps are designed for a range of operations and may be larger or smaller than needed for a particular application. Impeller trimming allows for customizing the pump's performance to meet these specific needs, ensuring that the pump operates at optimal efficiency.\n\n## Energy Savings and Environmental Impact\n\nTrimming the impeller is not only beneficial for energy savings but also contributes to environmental sustainability. Reduced energy consumption leads to lower greenhouse gas emissions. For every kilowatt-hour (kWh) saved by the pump, the reduction in power station output significantly decreases pollution. This correlation is particularly stark, with each kilowatt saved at the pump corresponding to approximately six kilowatts saved at the power station.\n\n## Traditional Methods of Impeller Trimming\n\n### Scaling Methods\n\nTraditional methods for impeller trimming typically involve scaling laws and empirical correlations derived from extensive testing and experience. These methods include:\n\n\n\n#### Constant-Area Scaling\n\nConstant-area scaling assumes that the trimmed impeller maintains a constant area, ensuring proportional changes in flow and head. This method involves adjusting the impeller diameter while maintaining the proportional relationship between the flow rate and head.\n\n$$\n\\text{constant-area scaling:} \\quad \\frac{Q'}{Q} = \\frac{D_2'}{D_2} \\frac{H'}{H} = \\left( \\frac{D_2'}{D_2} \\right)^2\n$$\n\n## Artificial Neural Networks for Impeller Trimming\n\nArtificial Neural Networks (ANNs) offer a robust alternative to traditional methods by leveraging large datasets to predict pump performance accurately. Unlike empirical methods, ANNs can model complex, non-linear relationships between variables, providing more precise predictions.\n\n### Advantages of Neural Networks\n\n- **Accuracy**: ANNs can learn from vast amounts of data, capturing intricate patterns and relationships that traditional methods might miss.\n- **Efficiency**: Once trained, ANNs can quickly predict performance outcomes for different impeller diameters, saving time and resources.\n- **Adaptability**: Neural networks can be updated with new data, continuously improving their predictive capabilities.\n\n### Neural Network Architecture\n\nThe architecture of the neural network plays a crucial role in its performance. Key components include:\n- **Input Layer**: Represents the features or variables used for prediction, such as flow rate and diameter.\n- **Hidden Layers**: Intermediate layers that process the inputs through weighted connections. The number of hidden layers and neurons per layer is optimized using hyperparameter tuning.\n- **Output Layer**: Provides the predicted performance metrics, such as head and power.\n\n### Training the Neural Network\n\nThe training process involves adjusting the weights of the network to minimize the error between predicted and actual values. This is achieved through backpropagation and optimization algorithms.\n\n### Hyperparameters and Unlearnable Parameters\n\nHyperparameters are settings that you adjust before training your neural network. These parameters influence the training process and the structure of the network. They are not learned from the data but are set by the user. In our code, we have chosen to optimize several hyperparameters, including:\n\n- **Number of neurons in the hidden layers**: This determines the capacity of the neural network to learn from the data. More neurons can capture more complex patterns but may also lead to overfitting if not managed properly.\n- **Training method**: This is specified by the choice of optimizer. In our code, we use the Levenberg-Marquardt (`trainlm` in MATLAB) optimizer for its efficiency in training feedforward networks.\n- **Number of epochs**: Epochs refer to the number of complete passes through the training dataset. Our code optimizes the number of epochs to ensure the model is well-trained without overfitting.\n- **Activation functions**: These functions define the output of each neuron. We experiment with different activation functions like `tansig` and `logsig` to find the best fit for our model.\n\n## Genetic Algorithm for Hyperparameter Optimization\n\nGenetic algorithms (GAs) are a class of optimization techniques inspired by the process of natural selection. They are particularly useful for optimizing complex problems with large search spaces, such as neural network hyperparameter tuning.\n\n### How Genetic Algorithms Work\n\n1. **Initialization**: A population of potential solutions (individuals) is generated. Each individual represents a set of hyperparameters.\n2. **Selection**: Individuals are selected based on their fitness, which is typically a function of how well they perform on a given task (e.g., predicting pump performance).\n3. **Crossover**: Pairs of individuals are combined to produce offspring. This process involves swapping parts of their hyperparameter sets to create new solutions.\n4. **Mutation**: Some offspring undergo random changes to introduce diversity into the population.\n5. **Evaluation**: The fitness of the new generation is evaluated, and the best individuals are selected for the next iteration.\n6. **Termination**: The algorithm repeats the selection, crossover, mutation, and evaluation steps until a stopping criterion is met (e.g., a predefined number of generations or a satisfactory fitness level).\n\n### Using Genetic Algorithms in Our Code\n\nIn our code, we use a genetic algorithm to optimize the hyperparameters of our neural network. The key steps involved are:\n\n1. **Define the Search Space**: We specify the range for each hyperparameter, such as the number of neurons in hidden layers, the number of epochs, and the indices for the training and activation functions.\n2. **Set Genetic Algorithm Options**: We configure the genetic algorithm with options like population size, maximum number of generations, crossover fraction, and fitness limit.\n3. **Evaluate Hyperparameters**: A fitness function evaluates the performance of each set of hyperparameters. It trains the neural network and computes the mean squared error (MSE) across training, validation, and testing datasets.\n4. **Optimize**: The genetic algorithm iteratively searches for the optimal hyperparameters by generating new populations, evaluating their fitness, and selecting the best-performing sets.\n\nHere is an outline of the relevant code:\n\n```matlab\n% Define bounds for the genetic algorithm optimization\nlower_bounds = [2,  13,    13, 1, 1];\nupper_bounds = [2,  300,    300, 2, 1];\n\n% Genetic algorithm options\ngaOptions = optimoptions('ga', ...\n    'PopulationSize', 17, ...\n    'MaxGenerations', 13, ...\n    'CrossoverFraction', 0.8, ...\n    'ConstraintTolerance', 0.000991, ...\n    'FitnessLimit', 0.000991, ...\n    'EliteCount', 2, ...\n    'Display', 'iter', ...\n    'UseParallel', true);\n\n% Optimization using genetic algorithm\n[optimalHyperParams, finalMSE] = ga(@(hyperParams) evaluateHyperparameters(hyperParams, x, t, randomSeed), ...\n    length(lower_bounds), [], [], [], [], lower_bounds, upper_bounds, [], gaOptions);\n```\n\nIn this section of the code, we set up the bounds for the hyperparameters and configure the genetic algorithm options. The `evaluateHyperparameters` function is called by the genetic algorithm to assess the performance of each set of hyperparameters, guiding the search towards the optimal solution.\n\nThe combination of neural networks and genetic algorithms provides a powerful approach for predicting pump performance with high accuracy and efficiency, leveraging advanced optimization techniques to fine-tune the model.\n## Genetic Algorithm for Hyperparameter Optimization\n\nA genetic algorithm (GA) is used to optimize the hyperparameters of the neural network. GA is a search heuristic that mimics the process of natural selection, making it effective for exploring large and complex search spaces.\n\n### Key Steps in Genetic Algorithm\n\n1. **Initialization**: Creating an initial population of potential solutions with random hyperparameters.\n2. **Selection**: Evaluating the fitness of each individual in the population based on the mean squared error (MSE) of the neural network's predictions.\n3. **Crossover**: Combining pairs of individuals to produce offspring with mixed characteristics, promoting the inheritance of good traits.\n4. **Mutation**: Introducing random changes to some individuals to maintain genetic diversity and explore new solutions.\n5. **Evaluation**: Assessing the performance of the new population and iterating through the selection, crossover, and mutation steps until convergence or a predefined number of generations is reached.\n\n## Implementation in MATLAB\n\nThe implementation of AI for impeller trimming was carried out using MATLAB. The scripts `main_04.m` and `QHforDiameters.m` are critical components of this implementation, leveraging optimized neural network architectures to predict pump performance based on different impeller diameters.\n\n### Script: main_04.m\n\nThe `main_04.m` script incorporates the following key steps:\n\n1. **Data Loading**: Loading datasets containing flow rate, head, diameter, and power metrics.\n2. **Network Training**: Training neural networks with optimized architectures to predict head and power based on flow rate and diameter.\n3. **Performance Evaluation**: Evaluating the trained networks on various performance metrics to ensure accuracy and reliability.\n4. **Visualization**: Generating 3D plots to visualize the relationship between flow rate, head, diameter, and power, showcasing the neural network predictions versus actual data.\n\n#### Key Functions and Their Roles\n\n- **train_nn**: This function trains the neural network using the provided data, returning the trained model and performance metrics.\n- **trim_diameters**: This function determines the optimal trimmed diameter based on the provided pump data and performance criteria.\n- **processDataAndVisualize**: This function processes the data and generates visualizations to compare neural network predictions with actual data points.\n\n#### Sample Code Snippet from `main_04.m`\n\nThe following MATLAB code snippet from `main_04.m` demonstrates how to load data, train neural networks, and visualize the results:\n\n```matlab\nclear; clc; clf; close all;\n\n% Load data\nload('filtered_QHD_table.mat');\nload('filtered_QDP_table.mat');\nload('deleted_QHD_table.mat');\nload('deleted_QDP_table.mat');\n\n% Extract data\nQH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';\nD = [filtered_QHD_table.Diameter_mm]';\nQD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';\nP = [filtered_QDP_table.Power_kW]';\n\n% Train on full dataset\n[trainedNetQHD, ~, ~, ~, ~] = train_nn([2, 16], 191, 'trainlm', QH, D, 4837);\n[trainedNetQDP, ~, ~, ~, ~] = train_nn([2, 7, 29, 17], 191, 'trainlm', QD, P, 4837);\n\n% Visualization\nprocessDataAndVisualize(QH', D', QD', P', trainedNetQHD, trainedNetQDP, 'figures');\n```\n\n### Script: QHforDiameters.m\n\nThe `QHforDiameters.m` script focuses on optimizing neural network hyperparameters for better performance prediction. It uses a genetic algorithm to find the optimal neural network architecture, ensuring accurate predictions for different impeller diameters.\n\n#### Key Steps in `QHforDiameters.m`\n\n1. **Initialization**: Loading data and initializing variables.\n2. **Hyperparameter Optimization**: Using a genetic algorithm to find the optimal neural network architecture.\n3. **Performance Evaluation**: Assessing the neural network's performance on the training and test datasets.\n4. **Visualization**: Plotting the predicted performance curves for different impeller diameters.\n\n#### Sample Code Snippet from `QHforDiameters.m`\n\nThe following MATLAB code snippet from `QHforDiameters.m` illustrates the process of optimizing neural network hyperparameters and visualizing the results:\n\n```matlab\nclear; clc; clf;\nload('filtered_QHD_table.mat');\nload('filtered_QDP_table.mat');\nload('deleted_QHD_table.mat');\nload('deleted_QDP_table.mat');\n\nQH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';\nD  = [filtered_QHD_table.Diameter_mm]';\n\nQH_beps=[deleted_QHD_table.FlowRate_m3h,deleted_QHD_table.Head_m]';\nD_beps\n\n=[deleted_QHD_table.Diameter_mm]';\n\nQD = [filtered_QDP_table.FlowRate_m3h,filtered_QDP_table.Diameter_mm]';\nP = [filtered_QDP_table.Power_kW]';\n\nQD_beps=[deleted_QDP_table.FlowRate_m3h,deleted_QDP_table.Diameter_mm]';\nP_beps=[deleted_QDP_table.Power_kW]';\n\n% User-specified random seed (optional)\nuserSeed = 4826;\n\n% Define a threshold for MSE to exit the loop early\nmseThreshold = 0.000199;\n\n% Initialize result matrix\nresult = [];\n\n% Find all distinct diameters in D\ndistinctDiameters = unique(D);\n\n% Weights for combining MSEs\nweightDiameter = 0.5;\nweightBeps = 0.5;\n\nfor dIdx = 1:length(distinctDiameters)\n    % Current diameter to remove\n    diameterToRemove = distinctDiameters(dIdx);\n    \n    % Remove the current diameter from QH and D\n    idxToKeep = D ~= diameterToRemove;\n    QH_filtered = QH(:, idxToKeep);\n    D_filtered = D(idxToKeep);\n    \n    % Calculate the number of epochs based on dataset size\n    maxEpochs = 1000 + floor(size(QH_filtered, 2) / 10);\n    \n    % Determine the number of hidden layers and neurons to search\n    hiddenLayers = [1:10];\n    neuronsPerLayer = [1:30];\n    \n    % Initialize the best MSE and corresponding architecture\n    bestMSE = Inf;\n    bestArch = [];\n    \n    % Random seed for reproducibility\n```\n\n## Results\n\n### QHD results table\n\n::: {#bd0eb5c7 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load the QHD results table\nqhd_results = pd.read_csv('QHD_results.csv')\n# Rename columns to ensure headers are displayed correctly\nqhd_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']\nqhd_results\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DiameterRemoved</th>\n      <th>AvgMSE</th>\n      <th>TrainPerformance</th>\n      <th>ValPerformance</th>\n      <th>TestPerformance</th>\n      <th>MSEDeletedDiameter</th>\n      <th>MSEBEPs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230</td>\n      <td>0.025035</td>\n      <td>0.006973</td>\n      <td>0.013087</td>\n      <td>0.063566</td>\n      <td>13.109880</td>\n      <td>0.887325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>240</td>\n      <td>0.002054</td>\n      <td>0.001905</td>\n      <td>0.002182</td>\n      <td>0.002146</td>\n      <td>0.174083</td>\n      <td>0.033176</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>250</td>\n      <td>0.003343</td>\n      <td>0.002433</td>\n      <td>0.004716</td>\n      <td>0.003316</td>\n      <td>0.626366</td>\n      <td>0.015352</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260</td>\n      <td>0.002077</td>\n      <td>0.001944</td>\n      <td>0.001985</td>\n      <td>0.002364</td>\n      <td>23.101650</td>\n      <td>0.330787</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### QDP results tabel\n\n::: {#51e9fa5f .cell execution_count=2}\n``` {.python .cell-code}\n# Load the QDP results table\nqdp_results = pd.read_csv('QDP_results.csv')\nqdp_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']\nqdp_results\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DiameterRemoved</th>\n      <th>AvgMSE</th>\n      <th>TrainPerformance</th>\n      <th>ValPerformance</th>\n      <th>TestPerformance</th>\n      <th>MSEDeletedDiameter</th>\n      <th>MSEBEPs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230</td>\n      <td>1.061458</td>\n      <td>1.074642</td>\n      <td>1.005925</td>\n      <td>1.097531</td>\n      <td>1.459545</td>\n      <td>0.504712</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>240</td>\n      <td>0.856944</td>\n      <td>0.717666</td>\n      <td>0.856781</td>\n      <td>1.062996</td>\n      <td>2.272058</td>\n      <td>0.855605</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>250</td>\n      <td>0.001608</td>\n      <td>0.001304</td>\n      <td>0.001949</td>\n      <td>0.001718</td>\n      <td>11.410656</td>\n      <td>0.001787</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260</td>\n      <td>0.001312</td>\n      <td>0.001193</td>\n      <td>0.001446</td>\n      <td>0.001355</td>\n      <td>3.709925</td>\n      <td>0.001861</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### QDH results tabel\n\n::: {#49231926 .cell execution_count=3}\n``` {.python .cell-code}\n# Load the QDH results table\nqdh_results = pd.read_csv('QDH_results.csv')\nqdh_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']\nqdh_results\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DiameterRemoved</th>\n      <th>AvgMSE</th>\n      <th>TrainPerformance</th>\n      <th>ValPerformance</th>\n      <th>TestPerformance</th>\n      <th>MSEDeletedDiameter</th>\n      <th>MSEBEPs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230</td>\n      <td>0.003924</td>\n      <td>0.002607</td>\n      <td>0.002910</td>\n      <td>0.006876</td>\n      <td>0.030449</td>\n      <td>0.001832</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>240</td>\n      <td>2.324643</td>\n      <td>2.171064</td>\n      <td>2.720251</td>\n      <td>2.155551</td>\n      <td>22.142540</td>\n      <td>4.378357</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>250</td>\n      <td>0.002601</td>\n      <td>0.001929</td>\n      <td>0.003371</td>\n      <td>0.002826</td>\n      <td>781.213228</td>\n      <td>201.866566</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260</td>\n      <td>0.002949</td>\n      <td>0.002372</td>\n      <td>0.004189</td>\n      <td>0.002559</td>\n      <td>5.875235</td>\n      <td>0.017837</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### errors and reductions\n\nhere we compare the best neural network we have with the traditional *constant area scaling methode form [weme paper need citation]* where \n\n```matlab\n% Loop through each column in QH_beps\nfor i = 1:5\n    d_real = D_beps(1, i); % Extracting d_real from D_beps\n\n    % Calculate d using constant_area_scaling\n    d_trimmed_cas_260 = constant_area_scaling(QH_beps(1, i), QH_beps(2, i), pump_data(5).Q, pump_data(5).H, pump_data(5).Diameter, 4);\n    percent_errors_cas_260(i) = abs((d_trimmed_cas_260 - d_real) / d_real) * 100;\n\n    % Calculate d using trim_diameters\n    d_trimmed_cas_nearest = trim_diameters(QH_beps(:, i), 'filtered_QHD_table.mat');\n    percent_errors_cas_nearest(i) = abs((d_trimmed_cas_nearest - d_real) / d_real) * 100;\n\n    % Calculate d using trainedNetQHD\n    d_trimmed_nn = bestNetQHD.net(QH_beps(:, i));\n    percent_errors_nn(i) = abs((d_trimmed_nn - d_real) / d_real) * 100;\n\n    % Calculate percent reduction in diameter\n    percent_reductions(i) = abs((d_real - d_trimmed_nn) / d_real) * 100;\nend\n\n```\n\nas we see here we do so in two different variants where in `percent_errors_cas_260` we feed to `constant_area_scaling` the last diameter the  $ 260\\:mm $ diameter as the base diameter and for all the test points which are the best effecincy points they will trimm form it and this gives greater error as you see below.\n\nwhile `trim_diameters` the function will find the nearst curve from the given 5 curves to trim againist it and this will reduce the error so much but it still higher that the nn by order of magnitude.\n\n::: {#b4bad9cd .cell execution_count=4}\n``` {.python .cell-code}\nerros_reductions = pd.read_csv('errors_and_reductions.csv')\nerros_reductions .columns = ['Index', 'Percent_Error_CAS_260', 'Percent_Error_CAS_Nearest', 'Percent_Error_NN', 'Percent_Reduction']\nerros_reductions \n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>Percent_Error_CAS_260</th>\n      <th>Percent_Error_CAS_Nearest</th>\n      <th>Percent_Error_NN</th>\n      <th>Percent_Reduction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>241.145202</td>\n      <td>3.261635</td>\n      <td>2.541009</td>\n      <td>2.541009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>241.798697</td>\n      <td>3.076349</td>\n      <td>3.588867</td>\n      <td>3.588867</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>238.978142</td>\n      <td>8.063994</td>\n      <td>3.085461</td>\n      <td>3.085461</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>228.293618</td>\n      <td>1.351752</td>\n      <td>2.207965</td>\n      <td>2.207965</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>245.536126</td>\n      <td>3.828971</td>\n      <td>2.299324</td>\n      <td>2.299324</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### final stats\n\n::: {#e65d4ede .cell execution_count=5}\n``` {.python .cell-code}\nfstats = pd.read_csv('final_statistics.csv')\nfstats.columns = ['MAE_Trim_Diameters', 'MAE_TrainedNetQHD', 'Count_Better_TrainedNetQHD', 'Count_Better_Trim_Diameters']\nfstats\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAE_Trim_Diameters</th>\n      <th>MAE_TrainedNetQHD</th>\n      <th>Count_Better_TrainedNetQHD</th>\n      <th>Count_Better_Trim_Diameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.91654</td>\n      <td>2.744525</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Figures\n\n\n::: {#gallery .column-page}\n:::\n\n\n## Conclusion\n\nThe use of Artificial Intelligence, particularly neural networks and genetic algorithms, provides a powerful tool for predicting the effects of impeller trimming on centrifugal pump performance. This approach offers significant advantages in terms of accuracy, efficiency, and adaptability, making it a superior alternative to traditional methods. By optimizing pump performance, we can achieve substantial energy savings and reduce environmental impact, contributing to a more sustainable industrial practice.\n\n\n\n---\n\n\n# QHforDiameters.m code docs\n\nthis file is the pre-step to our final file main_04.m\n\n\nThis MATLAB script `QHforDiameters.m` demonstrates an advanced approach to optimizing pump impeller diameters using neural networks. By systematically training\n\n and refining the network, we achieve a model capable of accurately predicting the optimal diameter, thereby improving pump efficiency. This methodology showcases the potential of AI in engineering applications, blending data-driven insights with practical engineering challenges.\n\n## Code Breakdown\n\n### Initial Setup\n\nThe script begins by clearing the workspace and loading several data files:\n\n```matlab\nclear; clc; clf;\nload('filtered_QHD_table.mat')\nload('filtered_QDP_table.mat')\nload('deleted_QHD_table.mat')\nload('deleted_QDP_table.mat')\n```\n\n- `filtered_QHD_table.mat` and `filtered_QDP_table.mat`: Contain the filtered data for flow rate (`Q`), head (`H`), and power (`P`) against diameters (`D`).\n- `deleted_QHD_table.mat` and `deleted_QDP_table.mat`: Contain the data points excluded from the filtered tables.\n\n### Data Preparation\n\nThe loaded data is then organized into matrices for further processing:\n\n```matlab\nQH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';\nD  = [filtered_QHD_table.Diameter_mm]';\n\nQH_beps = [deleted_QHD_table.FlowRate_m3h, deleted_QHD_table.Head_m]';\nD_beps = [deleted_QHD_table.Diameter_mm]';\n\nQD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';\nP = [filtered_QDP_table.Power_kW]';\n\nQD_beps = [deleted_QDP_table.FlowRate_m3h, deleted_QDP_table.Diameter_mm]';\nP_beps = [deleted_QDP_table.Power_kW]';\n```\n\nHere, we arrange the data into variables for flow rate and head (`QH`), diameters (`D`), and power (`P`).\n\n### Hyperparameter Optimization Setup\n\nWe define several key parameters for the optimization process:\n\n```matlab\nuserSeed = 4826;\nmseThreshold = 0.000199;\nresult = [];\ndistinctDiameters = unique(D);\n\nweightDiameter = 0.5;\nweightBeps = 0.5;\n```\n\n- `userSeed`: A seed for random number generation to ensure reproducibility.\n- `mseThreshold`: The mean squared error threshold for early stopping.\n- `distinctDiameters`: A unique set of diameters for which we'll optimize our neural network.\n\n### Optimization Loop\n\nThe core of the script involves iterating over each distinct diameter, removing it from the dataset, and training a neural network to predict the head (`H`) given the flow rate (`Q`) and the diameter (`D`):\n\n```matlab\nfor dIdx = 1:length(distinctDiameters)\n    diameterToRemove = distinctDiameters(dIdx);\n    indicesToRemove = find(D == diameterToRemove);\n    removedQH = QH(:, indicesToRemove);\n    removedD = D(indicesToRemove);\n    QH_temp = QH;\n    D_temp = D;\n    QH_temp(:, indicesToRemove) = [];\n    D_temp(:, indicesToRemove) = [];\n    \n    Qa = QH_temp(1,:);\n    Ha = QH_temp(2,:);\n    Q = QH_temp(1,:);\n    H = QH_temp(2,:);\n\n    lower_bounds = [2, 13, 13, 1, 1];\n    upper_bounds = [2, 300, 300, 2, 1];\n    prevCombinedMSE = inf;\n\n    for i = 1:20\n        [optimalHyperParamsH, finalMSEH, randomSeedH, bestTrainedNetH, error] = ...\n            optimizeNNForTrimmingPumpImpeller([QH_temp(1,:); D_temp], QH_temp(2,:), userSeed+i, lower_bounds, upper_bounds);\n\n        result(i, :) = [i, optimalHyperParamsH, finalMSEH, randomSeedH, error(1), error(2), error(3)];\n        predictedH = bestTrainedNetH([removedQH(1, :); removedD])';\n        mseDiameter = mean((removedQH(2, :)' - predictedH).^2 / sum(removedQH(2, :)));\n\n        predictedH_beps = bestTrainedNetH([QH_beps(1,:); D_beps])';\n        mseQH_beps = mean((QH_beps(2,:)' - predictedH_beps).^2 / sum(QH_beps(2,:)));\n\n        fprintf('Diameter %d, Iteration %d, MSE_Dia: %.6f,  MSE_beps: %.6f \\n', diameterToRemove, i, mseDiameter, mseQH_beps);\n\n        combinedMSE = weightDiameter * mseDiameter + weightBeps * mseQH_beps;\n        deltaMSE = prevCombinedMSE - combinedMSE;\n        \n        if deltaMSE > 0.01\n            adjustment = [0, 5, 15, 0, 0];\n        elseif deltaMSE > 0.001\n            adjustment = [0, 2, 10, 0, 0];\n        else\n            adjustment = [0, 1, 5, 0, 0];\n        end\n        \n        lower_bounds = max(lower_bounds, [2, optimalHyperParamsH(2), optimalHyperParamsH(3), 1, 1] - adjustment);\n        upper_bounds = min(upper_bounds, [2, optimalHyperParamsH(2), optimalHyperParamsH(3), 2, 1] + adjustment);\n        prevCombinedMSE = combinedMSE;\n\n        if (mseDiameter < mseThreshold) && (error(3) < 0.0199) && (mseQH_beps < mseThreshold)\n            fprintf('MSE for diameter %d is below the threshold. Exiting loop.\\n', diameterToRemove);\n            break;\n        end\n    end\nend\n```\n\n### Plotting Results\n\nThe script generates plots to visualize the performance of the neural network:\n\n```matlab\nfor diameterIndex = 1:length(desiredDiameters)\n    desiredDiameter = desiredDiameters(diameterIndex);\n    Dt = repmat(desiredDiameter, length(Qt), 1);\n    filteredQH = bestTrainedNetH([Qt; Dt'])';\n\n    legendLabel = strcat('Diameter: ', num2str(desiredDiameter), 'mm');\n    plot(Qt, filteredQH, 'DisplayName', legendLabel);\n    text(Qt(end), filteredQH(end), sprintf('%dmm', desiredDiameter), 'FontSize', 8, 'Color', 'black', 'BackgroundColor', 'white');\nend\n\nscatter(Qa', Ha', 'b', 'filled', 'DisplayName', 'Reference Points');\nxlabel('Q (m^3/h)');\nylabel('H (m)');\ntitle(['(Q, H) slices with Diameters, Removed Diameter: ' num2str(diameterToRemove) 'mm']);\nlegend;\nhold off;\n```\n\n### Saving Results\n\nThe results and trained networks are saved for further analysis:\n\n```matlab\nfilename = sprintf('../loop_05/01/nn_diameter-%d_iteration_%d_%d-%d-%d-%d-%d_mseDia-%d_test-%d.png', diameterToRemove, i, ...\n    optimalHyperParamsH(1), optimalHyperParamsH(2), optimalHyperParamsH(3), ...\n    optimalHyperParamsH(4), optimalHyperParamsH(5), mseDiameter, error(3));\nsaveas(gcf, filename);\n\nfilename = sprintf('../loop_05/01/nn_diameter-%d_iteration_%d_%d-%d-%d-%d-%d_mseDia-%d_test-%d.mat', diameterToRemove, i, ...\n    optimalHyperParamsH(1), optimalHyperParamsH(2), optimalHyperParamsH(3), ...\n    optimalHyperParamsH(4), optimalHyperParamsH(5), mseDiameter, error(3));\nsave(filename, 'bestTrainedNetH');\n```\n\n### Writing Results to CSV\n\nFinally, the results of the optimization are written to a CSV file for documentation:\n\n```matlab\nwritematrix([[\"Iteration\", \"Hidden Layer 1 Size\", \"Hidden Layer 2 Size\", \"Max Epochs\", ...\n    \"Training Function\", \"Activation Function\", \"Final MSE\", ...\n    \"Random Seed\", \"Training Error\", \"Validation Error\", \"Test Error\"]; result], './01/results_loop.csv');\n\ndisp('./01/Results saved to results_loop.csv');\n```\n\n### Supporting Functions\n\nSeveral supporting functions handle data loading, neural network optimization, and visualization:\n\n```matlab\nfunction [QH, D, QD, P] = loadData(dataPath)\n% Function to load data from specified path\n% ...\nend\n\nfunction [optimalHyperParams, finalMSE, randomSeed, bestTrainedNet, nnPerfVect] = optimizeNNForTrimmingPumpImpeller(x, t, userSeed, lowerBounds, upperBounds)\n% Function to optimize neural network hyperparameters using genetic algorithm\n% ...\nend\n\nfunction processDataAndVisualize(QH, D, QD, P, bestTrainedNetD, bestTrainedNetP, saveFigures)\n% Function to process data and generate visualizations\n% ...\nend\n```\n\n\n\n---\n\n\n# main_04.m\n\nThis MATLAB script  `main_04.m` exemplifies the application of neural networks in optimizing pump performance by predicting outcomes such as flow rate, head, and power. By training models on different subsets of data, it ensures robustness and generalization, leading to improved design and operational efficiency in pump systems. This project can be extended further by incorporating more advanced AI techniques and real-time data for continuous optimization.\n\n\n\nThe code performs the following key steps:\n1. Load and preprocess data.\n2. Train neural networks on the data.\n3. Evaluate the trained models.\n4. Use the trained models to predict and visualize performance under various conditions.\n\n### Details\n\n#### Initialization and Data Loading\n\n```matlab\nclear; clc; clf; close all;\n\n% Load data\nload('filtered_QHD_table.mat');\nload('filtered_QDP_table.mat');\nload('deleted_QHD_table.mat');\nload('deleted_QDP_table.mat');\n```\n- `clear; clc; clf; close all;` clears the workspace, command window, figure window, and closes any open figures.\n- The `load` commands import datasets from `.mat` files into the workspace. These datasets contain performance data of pumps under different conditions.\n\n#### Data Extraction\n\n```matlab\n% Extract data\nQH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';\nD = [filtered_QHD_table.Diameter_mm]';\nQH_beps = [deleted_QHD_table.FlowRate_m3h, deleted_QHD_table.Head_m]';\nD_beps = [deleted_QHD_table.Diameter_mm]';\nQD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';\nP = [filtered_QDP_table.Power_kW]';\nQD_beps = [deleted_QDP_table.FlowRate_m3h, deleted_QDP_table.Diameter_mm]';\nP_beps = [deleted_QDP_table.Power_kW]';\n```\n- The data is extracted into separate variables for easier handling.\n  - `QH` contains flow rate and head data.\n  - `D` contains diameter data.\n  - `QH_beps` and `D_beps` contain best efficiency point (BEP) data.\n  - `QD` contains flow rate and diameter data related to power.\n  - `P` contains power data.\n\n#### Creating Output Directories\n\n```matlab\n% Create output directories\noutput_dir = 'out_data';\nfigures_dir = fullfile(output_dir, 'figures');\nif ~exist(output_dir, 'dir')\n    mkdir(output_dir);\nend\nif ~exist(figures_dir, 'dir')\n    mkdir(figures_dir);\nend\n```\n- The code creates directories to save the output data and figures if they don't already exist.\n\n#### Neural Network Training Parameters\n\n```matlab\n% Hyperparameters based on latest optimization with GA\nrandomSeed = 4837;\nnn_QHD_size_matrix = [2, 16];\nnn_QDH_size_matrix = [2, 16];\nnn_QDP_size_matrix = [2, 7, 29, 17];\nmaxEpochs = 191;\ntrainFcn = 'trainlm';\n```\n- The hyperparameters for the neural networks are defined, including the random seed for reproducibility, network architecture (size of hidden layers), number of epochs for training, and the training function (`trainlm` - Levenberg-Marquardt).\n\n#### Training Neural Networks on Full Dataset\n\n```matlab\n[trainedNetQHD, avgMSEsQHD, trainPerformanceQHD, valPerformanceQHD, testPerformanceQHD] = train_nn(nn_QHD_size_matrix, maxEpochs, trainFcn, QH, D, randomSeed);\n[trainedNetQDH, avgMSEsQDH, trainPerformanceQDH, valPerformanceQDH, testPerformanceQDH] = train_nn(nn_QDH_size_matrix, maxEpochs, trainFcn, [QH(1,:); D], QH(2,:), randomSeed);\n[trainedNetQDP, avgMSEsQDP, trainPerformanceQDP, valPerformanceQDP, testPerformanceQDP] = train_nn(nn_QDP_size_matrix, maxEpochs, trainFcn, QD, P, randomSeed);\n```\n- Neural networks are trained on the full dataset for each model (QHD, QDH, and QDP) using the specified parameters.\n\n#### Initializing Results Tables and Logs\n\n```matlab\nQHD_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});\nQDP_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});\nQDH_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});\n\nlogs = {};  % Initialize logs\n```\n- Results tables and logs are initialized to store the performance metrics and log messages during the training process.\n\n#### Weights for Error Calculation\n\n```matlab\nweights = struct('train', 0.05, 'val', 0.05, 'test', 0.35, 'deleted_diameter', 0.45, 'beps', 0.1);\n```\n- Weights are defined to calculate a weighted score for model performance, giving different importance to training, validation, test errors, and errors on specific data points.\n\n#### Best Neural Network Initialization\n\n```matlab\nbestNetQHD = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);\nbestNetQDP = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);\nbestNetQDH = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);\n```\n- Structures are initialized to store the best neural networks for each model based on their performance.\n\n#### Function to Compute Weighted Score\n\n```matlab\ncompute_score = @(trainPerf, valPerf, testPerf, mseDeleted, mseBEPS, weights) ...\n    weights.train * trainPerf + weights.val * valPerf + weights.test * testPerf + weights.deleted_diameter * mseDeleted + weights.beps * mseBEPS;\n```\n- A function is defined to compute the weighted score for model performance based on the defined weights.\n\n### Training with Different Diameters Hidden (QHD and QDH)\n\n```matlab\ndistinctDiametersQHD = unique(D);\nfor dIdx = 1:length(distinctDiametersQHD)\n    diameterToRemove = distinctDiametersQHD(dIdx);\n    indicesToRemove = find(D == diameterToRemove);\n    removedQH = QH(:, indicesToRemove);\n    removedD = D(indicesToRemove);\n    QH_temp = QH;\n    D_temp = D;\n    QH_temp(:, indicesToRemove) = [];\n    D_temp(:, indicesToRemove) = [];\n\n    try\n        [trainedNetQHD_temp, avgMSEsQHD_temp, trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp] = train_nn(nn_QHD_size_matrix, maxEpochs, trainFcn, QH_temp, D_temp, randomSeed);\n        mse_deleted_diameter = perform(trainedNetQHD_temp, removedD, trainedNetQHD_temp(removedQH));\n        mse_beps = perform(trainedNetQHD_temp, D_beps, trainedNetQHD_temp(QH_beps));\n        logs{end+1} = ['Trained nn_QHD_temp on dataset without diameter ' num2str(diameterToRemove) ' successfully.'];\n\n        score = compute_score(trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp, mse_deleted_diameter, mse_beps, weights);\n\n        QHD_results = [QHD_results; {diameterToRemove, avgMSEsQHD_temp, trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp, mse_deleted_diameter, mse_beps}];\n\n        if score < bestNetQHD.score\n            bestNetQHD.net = trainedNetQHD_temp;\n            bestNetQHD.diameter = diameterToRemove;\n            bestNetQHD.score = score;\n            bestNetQHD.trainPerformance = trainPerformanceQHD_temp;\n            bestNetQHD.valPerformance = valPerformanceQHD_temp;\n            bestNetQHD.testPerformance = testPerformanceQHD_temp;\n        end\n\n        figure;\n        plot(QH(1,:), QH(2,:), 'bo', 'DisplayName', 'Original Data');\n        hold on;\n        plot(QH_temp(1,:), trainedNetQHD_temp([QH_temp(1,:); D_temp]), 'r*', 'DisplayName', 'Trained Net Predictions');\n        plot(removedQH(1,:), removedQH(2,:), 'gx', 'DisplayName', 'Removed Diameter Data\n\n');\n        hold off;\n        title(['QHD Model without Diameter ' num2str(diameterToRemove)]);\n        legend;\n        saveas(gcf, fullfile(figures_dir, ['QHD_wo_diameter_' num2str(diameterToRemove) '.png']));\n\n    catch ME\n        logs{end+1} = ['Error training nn_QHD_temp on dataset without diameter ' num2str(diameterToRemove) ': ' ME.message];\n    end\nend\n```\n- The above loop trains the QHD model by leaving out one diameter at a time and evaluates the performance on the remaining data. It then calculates the weighted score and updates the best model if the current one performs better.\n- Similar loops are used for QDH and QDP models.\n\n### Finalizing and Saving Results\n\n```matlab\n% Save results and best networks\nsave(fullfile(output_dir, 'QHD_results.mat'), 'QHD_results');\nsave(fullfile(output_dir, 'QDP_results.mat'), 'QDP_results');\nsave(fullfile(output_dir, 'QDH_results.mat'), 'QDH_results');\nsave(fullfile(output_dir, 'bestNetQHD.mat'), 'bestNetQHD');\nsave(fullfile(output_dir, 'bestNetQDP.mat'), 'bestNetQDP');\nsave(fullfile(output_dir, 'bestNetQDH.mat'), 'bestNetQDH');\n```\n- The results and the best models are saved to files for future use and analysis.\n\n\n\n\n---\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}