[
  {
    "objectID": "index.html#the-website-will-host",
    "href": "index.html#the-website-will-host",
    "title": "Home",
    "section": "the website will host",
    "text": "the website will host\n\nArticles & Reports\nPresentations\nInteractive Docs\nWebsites\nBooks"
  },
  {
    "objectID": "data/writings/index.html",
    "href": "data/writings/index.html",
    "title": "AI Applications in Pumps",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nShort Paper\n\n\nA Short Subtitle\n\n\nThis paper presents a genetic algorithm (GA) methodology to optimize neural network hyperparameters in the context of pump impeller trimming. Impeller trimming, a process involving modifications to pump impeller geometry, traditionally requires expert knowledge and empirical methods to achieve the desired performance. The use of neural networks (NNs) provides an automated approach to improve the impeller trimming process based on input data and performance outcomes. The proposed method uses a GA to identify the optimal NN hyperparameters, such as hidden layer size, training function, activation function, and maximum epochs, aiming to minimize the mean squared error (MSE) between the network’s predictions and the actual target outcomes. This paper discusses the implementation details of the optimization process and explains the key components and their significance. \n\n\n\n\n\nApr 15, 2024\n\n\nMohamed Farid Khalil, Mohammed Twheed Khater, Seif Ibrahim Hassan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html",
    "title": "Short Paper",
    "section": "",
    "text": "Pump impeller trimming is a critical procedure in optimizing pump performance for specific applications. It involves modifying the impeller’s geometry to achieve desired hydraulic characteristics such as head, flow rate, and efficiency. Traditionally, the process has been dependent on empirical methods and engineering expertise. However, the introduction of artificial neural networks (NNs) offers a data-driven approach to automate and enhance impeller trimming.\nNNs excel at modeling complex relationships between input data and desired outputs. In the case of pump impeller trimming, the input data (x) can include geometric parameters of the impeller, while the target data (t) can consist of pump performance metrics. By training an NN on a dataset of impeller designs and performance outcomes, the network can learn to predict new impeller performance based on their geometries.\nAchieving optimal NN performance requires selecting appropriate hyperparameters, which influence network architecture and the learning process. Key NN hyperparameters include the size of the hidden layer, the training function for weight updates, the activation function introducing non-linearity, and the maximum number of training epochs."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#define-options",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#define-options",
    "title": "Short Paper",
    "section": "Define Options",
    "text": "Define Options\nDefine the set of available training functions (e.g., Levenberg-Marquardt, Bayesian Regularization) and activation functions (e.g., tansigmoid, logarithmic sigmoid)."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#nested-evaluation-function",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#nested-evaluation-function",
    "title": "Short Paper",
    "section": "Nested Evaluation Function",
    "text": "Nested Evaluation Function\nImplement a nested function evaluateHyperparameters(hyperParams, x, t, randomSeed) to evaluate a candidate hyperparameter set (hyperParams), input data (x), and target data (t). This function: - Extracts individual hyperparameters (hidden layer size, maximum epochs, training and activation function indices). - Defines an NN architecture with the given hyperparameters. - Splits data into training, validation, and testing sets. - Trains the NN using the training data and specified training function. - Evaluates the NN on validation data and calculates MSE. - Returns the calculated MSE."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#random-seed-and-bounds",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#random-seed-and-bounds",
    "title": "Short Paper",
    "section": "Random Seed and Bounds",
    "text": "Random Seed and Bounds\nset a random seed for reproducibility and define bounds for each hyperparameter based on practical considerations and prior knowledge.\n\nImportance of Random Seed:\n\nReproducibility: Setting a random seed ensures that the neural network training process is reproducible. This allows for consistent results across different runs and helps in comparing models fairly.\nComparison: When testing different hyperparameters or models, using the same random seed allows for a direct comparison of their performance."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#ga-options",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#ga-options",
    "title": "Short Paper",
    "section": "GA Options",
    "text": "GA Options\nConfigure GA options using optimoptions('ga'), specifying parameters such as maximum allowed time."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#perform-optimization",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#perform-optimization",
    "title": "Short Paper",
    "section": "Perform Optimization",
    "text": "Perform Optimization\nExecute the GA using ga from MATLAB’s Optimization Toolbox. This function uses the evaluateHyperparameters function as the objective function to minimize, along with the number of hyperparameters, bounds, and GA options."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#extract-and-round",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#extract-and-round",
    "title": "Short Paper",
    "section": "Extract and Round",
    "text": "Extract and Round\nObtain the final optimized hyperparameter set and round them (if necessary) for practical NN implementation."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#report-results",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#report-results",
    "title": "Short Paper",
    "section": "Report Results",
    "text": "Report Results\nReport the optimized hyperparameters, final MSE, random seed used, and total elapsed time of the optimization process."
  },
  {
    "objectID": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#references",
    "href": "data/writings/posts/2024-04-15-trimming-draft-01/index.html#references",
    "title": "Short Paper",
    "section": "References",
    "text": "References\n\n\nDirac, P.A.M., 1953. The Lorentz transformation and absolute time. Physica 19, 888–896. https://doi.org/10.1016/S0031-8914(53)80099-6\n\n\nFeynman, R.P., Vernon Jr., F.L., 1963. The theory of a general quantum system interacting with a linear dissipative system. Annals of Physics 24, 118–173. https://doi.org/10.1016/0003-4916(63)90068-X"
  }
]