[
  {
    "objectID": "index.html#beyond-trial-and-error-a-machine-learning-approach-to-optimal-centrifugal-pump-impeller-trimming",
    "href": "index.html#beyond-trial-and-error-a-machine-learning-approach-to-optimal-centrifugal-pump-impeller-trimming",
    "title": "Home",
    "section": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "text": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming\nThis paper proposes a novel method for optimizing pump impeller trimming using machine learning. Traditionally, this process relies on expertise and can be time-consuming. The proposed approach utilizes artificial neural networks (NNs) to predict impeller diameter and pump power based on desired operating conditions. A genetic algorithm (GA) then optimizes the NN for accurate predictions, leading to potentially faster and more efficient pump impeller trimming.\nKeywords: pump impeller trimming, machine learning, neural networks, genetic algorithm, neural network optimization\n\nLearn more »"
  },
  {
    "objectID": "data/writings/index.html",
    "href": "data/writings/index.html",
    "title": "AI Applications in Pumps",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBeyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming\n\n\nA Genetic Algorithm Hyperparameter Optimization Approach\n\n\nThis paper proposes a genetic algorithm (GA)-based methodology to optimize hyperparameters for artificial neural networks (NNs) applied to pump impeller trimming. Traditionally, impeller trimming relies on expert knowledge and empirical methods, leading to time-consuming and potentially suboptimal outcomes. This work introduces a data-driven approach using NNs trained to predict the impeller diameter (D) and pump power (P), which can be calculated from flow rate (Q), density (\\(\\rho\\)), head (H), and efficiency (\\(\\eta\\)) using the equation P = (Q * \\(\\rho\\) * g * H) / \\(\\eta\\). based on the desired operating point (flow rate (Q) and head (H)). A GA is employed to identify the optimal NN hyperparameters, including hidden layer size, training function, activation function, and maximum epochs. The goal is to minimize the mean squared error (MSE) between the network’s predictions and the actual performance data. The paper details the implementation of the GA optimization process and discusses the key components and their significance in achieving optimal impeller trimming through NN predictions. \n\n\n\n\n\nApr 15, 2024\n\n\nMohamed Farid Khalil, Mohammed Twheed Khater, Seif Ibrahim Hassan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html",
    "href": "data/writings/posts/trimming_15-4-2024/index.html",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "",
    "text": "MS WordPDF (elsevier)"
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html#methodology",
    "href": "data/writings/posts/trimming_15-4-2024/index.html#methodology",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "Methodology",
    "text": "Methodology\nThis section outlines the methodology employed to optimize the hyperparameters of an artificial neural network (NN) for predicting pump impeller diameter (D) in the context of impeller trimming. The optimization process utilizes a genetic algorithm (GA) implemented using the MATLAB GA toolbox.\nData Acquisition and Preprocessing:\nThe methodology leverages a dataset containing impeller geometries and their corresponding performance data, including flow rate (Q), head (H), diameter (D), and pump power (P). While the dataset doesn’t include a direct measurement of efficiency (\\(\\eta\\)), it can be calculated using the provided information and the following equation:\n\\[\nP = \\frac{Q*\\rho*g*H}{\\eta}\n\\]\nwhere:\n\n\\(\\eta\\) is the pump efficiency (-)\n\\(\\rho\\) is the fluid density \\((kg/m^3)\\)\n\\(g\\) is the acceleration due to gravity \\((m/s^2)\\)\n\nThis data can be obtained from various sources, such as experimental measurements, computational fluid dynamics (CFD) simulations, or a combination of both. Our code (refer to the provided MATLAB code for specific details)1 performs the following data preprocessing steps:\n\nData Cleaning: The data is inspected for missing values or outliers. Missing values can be addressed through techniques like imputation or deletion, while outliers may require further investigation or removal if they significantly impact the training process.\nNormalization: The data is normalized to a specific range (often -1 to 1 or 0 to 1) using techniques like mapminmax in MATLAB. This helps ensure the gradients calculated during backpropagation have a more consistent magnitude, facilitating faster convergence and avoiding local minima during training.\n\nNeural Network Architecture:\nThe proposed NN architecture utilizes a supervised learning approach for regression. The specific details in the code will determine the exact structure, but a typical configuration might involve:\n\nInput Layer: This layer consists of two neurons, corresponding to the input features: flow rate (Q) and head (H).\nHidden Layer(s): One or more hidden layers are employed to learn complex relationships between the input and output data. The GA will optimize the number of neurons in the hidden layer(s) based on performance.\nOutput Layer: The output layer contains a single neuron responsible for predicting the impeller diameter (D).\n\nThe activation functions used in each layer will also be optimized by the GA. The code specifically uses two common choices for activation functions in regression tasks:\n\nSigmoid (Logistic Function): This function takes the form f(x) = 1 / (1 + exp(-x)). It squashes the input values between 0 and 1, introducing non-linearity into the network. Mathematically, the sigmoid function can be expressed as: \\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nTanh (Hyperbolic Tangent Function): This function takes the form f(x) = (tanh(x)) = (e^x - e^{-x}) / (e^x + e^{-x}). It squashes the input values between -1 and 1, providing a wider range of output compared to the sigmoid function. Mathematically, the tanh function can be expressed as: \\[\nf(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\n\nBoth sigmoid and tansig (which is mathematically equivalent to tanh) functions introduce non-linearity into the network, allowing it to model complex relationships between the input and output data. The GA will evaluate the performance of the NN using different activation functions (sigmoid and tansig) and select the one that leads to the minimal MSE between predicted and actual data.\nThe tansig function used in your code is defined as: \\[\nf(x) = tansig(x) = \\frac{2}{1 + exp(-2x)} - 1\n\\]\nGenetic Algorithm (GA) for Hyperparameter Optimization:\nThe GA serves as the core optimization engine, searching for the optimal combination of NN hyperparameters that minimizes the mean squared error (MSE) between the network’s predicted diameter (D) and the actual values in the training data. and here is some glimpses on how in general it works:\n\nPopulation Initialization: The GA starts with a population of individuals (candidate hyperparameter sets). Each individual represents a unique configuration for the NN, including hidden layer size, training function, activation function, and maximum epochs.\nFitness Evaluation: Each individual in the population undergoes evaluation. The code trains an NN using the specific hyperparameters encoded in the individual’s chromosome. The resulting NN’s performance is then measured by calculating the MSE between its predicted diameter (D) and the actual values on a validation dataset (a portion of the original data set aside for evaluation).\nSelection: Based on the fitness values (MSE), the GA selects a subset of individuals with superior performance (low MSE) to become parents for the next generation.\nCrossover: Selected parent individuals undergo crossover, where portions of their genetic material (hyperparameter configurations) are exchanged to create new offspring (candidate solutions).\nMutation: A small probability of mutation is introduced to introduce random variations in the offspring’s chromosomes. This helps maintain genetic diversity and explore new regions of the search space.\nReplacement and Termination: The new generation of offspring replaces a portion of the lower-performing individuals in the population. The GA iterates through these steps until a stopping criterion is met, such as reaching a maximum number of generations or achieving a desired level of convergence (minimum MSE).\n\nThe code (refer to the MATLAB code for specific details) will implement the specific selection, crossover, and mutation operators used in the GA and all of this is handled by ga the MATLAB toolbox.\nTraining and Validation:\nThe final, selected hyperparameter configuration from the GA is used to train a new NN model. This model is trained on a separate training dataset, excluding the data used for validation during the GA optimization process. The trained model is then evaluated on a hold-out test dataset to assess its generalizability and prediction accuracy on unseen data.\nPerformance Evaluation:\nThe performance of the trained NN model is evaluated based on various metrics, including:\n\nMean Squared Error (MSE): This metric measures the average squared difference between the predicted impeller diameters (D) and the actual values in the test dataset. A lower MSE indicates better prediction accuracy.\nCoefficient of Determination (R-squared): This metric indicates the proportion of the variance in the actual diameter data explained by the NN model’s predictions. A value closer to 1 signifies a stronger correlation between the predicted and actual values.\nVisualization Techniques: Visualizations such as scatter plots comparing predicted vs. actual data."
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html#footnotes",
    "href": "data/writings/posts/trimming_15-4-2024/index.html#footnotes",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ngithub link to our code:https://github.com/MohammedTwheed/trimming-code ↩︎\nwe will find the dataset we used at: https://github.com/MohammedTwheed/trimming-code/tree/main/training-data↩︎"
  }
]