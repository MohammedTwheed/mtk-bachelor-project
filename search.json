[
  {
    "objectID": "index.html#beyond-trial-and-error-a-machine-learning-approach-to-optimal-centrifugal-pump-impeller-trimming",
    "href": "index.html#beyond-trial-and-error-a-machine-learning-approach-to-optimal-centrifugal-pump-impeller-trimming",
    "title": "Home",
    "section": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "text": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming\nThis paper proposes a novel method for optimizing pump impeller trimming using machine learning. Traditionally, this process relies on expertise and can be time-consuming. The proposed approach utilizes artificial neural networks (NNs) to predict impeller diameter and pump power based on desired operating conditions. A genetic algorithm (GA) then optimizes the NN for accurate predictions, leading to potentially faster and more efficient pump impeller trimming.\nKeywords: pump impeller trimming, machine learning, neural networks, genetic algorithm, neural network optimization\n\nLearn more »"
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "",
    "text": "PDF\n\n\nMS Word"
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#download-this-doc-as-pdf-or-word",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#download-this-doc-as-pdf-or-word",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "",
    "text": "PDF\n\n\nMS Word"
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#introduction",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#introduction",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "Introduction",
    "text": "Introduction\nSymbolic regression, a technique used to identify mathematical expressions that best fit a given dataset, is a complex task often tackled using genetic programming (GP). Genetic programming mimics natural evolution to optimize candidate solutions, represented as mathematical expressions. This review explores the implementation of a genetic algorithm for symbolic regression using MATLAB, delving into the detailed structure of the genetic algorithm and the representation of mathematical expressions as tree structures. The following sections will elaborate on the genetic algorithm’s key components, the tree structure for representing mathematical expressions, and the nuances of the MATLAB implementation."
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#genetic-algorithm-implementation",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#genetic-algorithm-implementation",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "Genetic Algorithm Implementation",
    "text": "Genetic Algorithm Implementation\nThe genetic algorithm (GA) for symbolic regression involves several critical steps: population initialization, selection, crossover, mutation, and fitness evaluation. Each of these steps is meticulously implemented in MATLAB to evolve mathematical expressions that best fit the provided data.\n\n1. Population Initialization\nPopulation initialization is achieved using the ramped half-and-half method, which ensures a diverse initial population. This method generates trees with varying depths and structures, promoting genetic diversity.\nfunction pop = initialPopulation(pop_size, max_depth, terminal_set, function_set)\n    pop = cell(1, pop_size);\n    for i = 1:pop_size\n        method = 'full';\n        if i &gt; (pop_size / 2)\n            method = 'grow';\n        end\n        pop{i} = growTree(max_depth, method, 0, terminal_set, function_set);\n    end\n    pop = pop(randperm(length(pop)));  % Randomize the population\nend\nThe growTree function generates individual trees using either the “full” method, creating fully populated trees to the maximum depth, or the “grow” method, generating trees with random depths.\n\n\n2. Selection\nTournament selection is employed to choose individuals for reproduction. This method ensures that individuals with better fitness are more likely to be selected, driving the population towards better solutions.\nfunction selected = tournamentSelection(pop, tour_size, x, y, rf, terminal_set)\n    selected_ind = randperm(length(pop), tour_size);\n    selected = cell(tour_size, 2);\n    for i = 1:tour_size\n        tree = pop{selected_ind(i)};\n        fitness = computeError(tree, x, y, rf, terminal_set);\n        selected{i, 1} = tree;\n        selected{i, 2} = fitness;\n    end\n    selected = sortrows(selected, 2);  % Sort based on fitness (ascending)\nend\nTournament selection enhances the likelihood of selecting fitter individuals while maintaining some level of genetic diversity, which is crucial for avoiding local optima.\n\n\n3. Crossover\nCrossover involves exchanging subtrees between pairs of parent trees to produce offspring with mixed characteristics. This operation introduces variability and combines the strengths of both parents.\nfunction [tree1, tree2] = crossover(tree1, tree2, c_rate)\n    if rand() &lt;= c_rate\n        swap_point1 = randi([0, countNodes(tree1) - 1]);\n        swap_point2 = randi([0, countNodes(tree2) - 1]);\n        subtree1 = getSubtree(tree1, swap_point1);\n        subtree2 = getSubtree(tree2, swap_point2);\n        tree1 = setSubtree(tree1, swap_point1, subtree2);\n        tree2 = setSubtree(tree2, swap_point2, subtree1);\n    end\nend\n\n\n4. Mutation\nMutation introduces random changes in the tree structure, which is essential for maintaining genetic diversity and exploring new solutions.\nfunction tree = mutate(tree, m_rate, max_depth, terminal_set, function_set)\n    if rand() &lt;= m_rate\n        mut_point = randi([0, countNodes(tree) - 1]);\n        subtree = growTree(max_depth, 'grow', 0, terminal_set, function_set);\n        tree = setSubtree(tree, mut_point, subtree);\n    end\nend\n\n\n5. Fitness Evaluation\nThe fitness of each tree is evaluated based on its ability to predict target values accurately. The fitness metric used is the root mean square error (RMSE), which quantifies the difference between the predicted and actual values.\nfunction error = computeError(tree, x, y, rf, terminal_set)\n    error_sum = 0;\n    n = length(x);\n    for i = 1:n\n        error_sum = error_sum + (computeTree(tree, x(i), y(i), terminal_set) - rf(i))^2;\n    end\n    error = sqrt(error_sum / n);\nend"
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#tree-structure-for-mathematical-expressions",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#tree-structure-for-mathematical-expressions",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "Tree Structure for Mathematical Expressions",
    "text": "Tree Structure for Mathematical Expressions\nIn genetic programming, mathematical expressions are represented as tree structures, where nodes represent functions or terminals. Terminals can be variables (e.g., x, y) or constants (e.g., 1, 2), while functions are operators (e.g., +, -, *, /).\n\nTree Representation\nEach tree node is a structure with a value (operator or terminal) and pointers to its left and right child nodes.\nfunction node = createNode(value, left, right)\n    node = struct('value', value, 'left', left, 'right', right);\nend\n\n\nTree Generation\nTrees are generated using the growTree function, which recursively builds trees up to a specified maximum depth. The tree generation can follow the “full” method, creating fully populated trees, or the “grow” method, producing trees with variable depths.\nfunction tree = growTree(max_depth, method, depth, terminal_set, function_set)\n    if strcmp(method, 'full') && depth &lt; max_depth\n        left = growTree(max_depth, method, depth + 1, terminal_set, function_set);\n        right = growTree(max_depth, method, depth + 1, terminal_set, function_set);\n        value = drawValue('f', terminal_set, function_set);\n    elseif strcmp(method, 'grow') && depth &lt; max_depth\n        value = drawValue('tf', terminal_set, function_set);\n        if any(strcmp(value, terminal_set))\n            tree = createNode(value, [], []);\n            return;\n        end\n        left = growTree(max_depth, method, depth + 1, terminal_set, function_set);\n        right = growTree(max_depth, method, depth + 1, terminal_set, function_set);\n    else\n        value = drawValue('t', terminal_set, function_set);\n        left = [];\n        right = [];\n    end\n    tree = createNode(value, left, right);\nend\n\n\nTree Evaluation\nThe computeTree function recursively evaluates the tree by computing the value of each subtree and applying the respective operator.\nfunction result = computeTree(tree, x, y, terminal_set)\n    if any(strcmp(tree.value, terminal_set))\n        result = eval(tree.value);\n    else\n        left_val = computeTree(tree.left, x, y, terminal_set);\n        right_val = computeTree(tree.right, x, y, terminal_set);\n        if strcmp(tree.value, '/') && right_val == 0\n            result = 1; % Avoid division by zero\n        else\n            result = eval([num2str(left_val) tree.value num2str(right_val)]);\n        end\n    end\nend"
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#matlab-implementation-details",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#matlab-implementation-details",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "MATLAB Implementation Details",
    "text": "MATLAB Implementation Details\nThe MATLAB code employs several programming techniques to ensure the efficient implementation of the genetic algorithm:\n\nModular Functions\nThe code is organized into modular functions, each responsible for a specific task, enhancing readability and maintainability.\n\n\nRandomization\nFunctions such as randperm and randi introduce randomness in the selection, crossover, and mutation processes, which is vital for maintaining genetic diversity.\n\n\nStructured Logging\nExecution progress and timing are logged using the logMessage function, providing valuable insights into the algorithm’s performance.\nfunction logMessage(message)\n    timestamp = datestr(now, 'yyyy-mm-dd HH:MM:SS');\n    fprintf('[%s] %s\\n', timestamp, message);\nend\n\n\nVisualization\n\n\n\n\n\n\nFigure 1: exanple plot with 200 generations\n\n\n\nThe plot visualizes the evolutionary process of symbolic regression using a genetic algorithm , highlighting the improvement in fitness (error reduction) and changes in solution complexity (tree depth) across generations. This approach aims to find mathematical expressions (symbolic models) that best fit the training data (A4_trainingSamples.txt). Each line provides critical insights into the algorithm’s performance and the nature of solutions generated over time.\nThe plotFigure function visualizes the evolution of fitness and tree depth over generations, aiding in the interpretation of the algorithm’s progress.\nfunction plotFigure(best_fit, avg_fit, worst_fit, tree_depth, gen)\n    generations = 10:10:gen;\n    figure;\n    yyaxis left\n    plot(generations, best_fit, 'r', generations, avg_fit, 'g', generations, worst_fit, 'b');\n    xlabel('Generation');\n    ylabel('Fitness');\n    legend('Best', 'Average', 'Worst');\n    \n    yyaxis right\n    plot(generations, tree_depth, 'c');\n    ylabel('Average Tree Depth');\n    legend('Tree Depth');\n    \n    title('Evolution of Fitness and Tree Depth');\nend\nPlot Explained:\nFitness Evolution (Left Y-axis):\nRed Line (Best Fitness): Represents the fitness (error) of the best individual in each generation. Green Line (Average Fitness): Shows the average fitness (error) of the population in each generation. Blue Line (Worst Fitness): Indicates the fitness (error) of the worst individual in each generation. These lines show how the fitness (error) values change over generations. The goal of the algorithm is typically to minimize this fitness value, as it represents the error in predicting the output (rf) based on the input (x, y).\nTree Depth Evolution (Right Y-axis):\nCyan Line (Average Tree Depth): Displays the average depth of the trees in the population for each generation. The depth of the trees is an indicator of their complexity. Genetic programming often evolves trees of varying depths, and monitoring the average tree depth helps in understanding the complexity of the solutions found over generations.\nX-axis (Generation):\nEach point on the x-axis represents a specific generation number (10:10:gen), where gen is the total number of generations specified in the code. Interpretation: Fitness Lines (Red, Green, Blue): Ideally, you want to see the red (best fitness) line decreasing over generations, indicating improvement in the best solution found. The green (average fitness) and blue (worst fitness) lines provide context on the overall performance of the population.\nTree Depth Line (Cyan): Monitoring the tree depth helps in understanding if the algorithm is converging towards simpler or more complex solutions. An increase in average tree depth might indicate overfitting or increasing complexity of solutions.\nSpecific Colors in the Plot: Green Line: Represents the average fitness (error) of the population. Blue Line: Represents the worst fitness (error) in the population. Cyan Line: Represents the average depth of trees in the population. These lines collectively show how the genetic algorithm progresses in terms of fitness (error minimization) and the complexity of the evolved solutions (average tree depth).\nThe plot helps in understanding how the fitness of the population improves over time and how the complexity of the solutions (tree depth) changes."
  },
  {
    "objectID": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#conclusion",
    "href": "data/writings/posts/2024-06-23-symbolic-regression-translation-to-matlab/index.html#conclusion",
    "title": "Exploartion_17 : Tree-Based Genetic Programming for Polynomial Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThe MATLAB implementation of genetic programming\nfor symbolic regression is a robust approach that leverages the power of evolutionary algorithms to discover optimal mathematical expressions. Key features include structured functions, effective randomization, detailed logging, and clear visualization. This review highlights the importance of genetic diversity, modular programming, and the use of tree structures for representing mathematical expressions. These elements collectively enhance the effectiveness of symbolic regression using genetic algorithms.\n\nReferences\nthis part is still uderconstruction to be continued later"
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html",
    "href": "data/writings/posts/trimming_15-4-2024/index.html",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "",
    "text": "MS WordPDF (elsevier)"
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html#methodology",
    "href": "data/writings/posts/trimming_15-4-2024/index.html#methodology",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "Methodology",
    "text": "Methodology\nThis section outlines the methodology employed to optimize the hyperparameters of an artificial neural network (NN) for predicting pump impeller diameter (D) in the context of impeller trimming. The optimization process utilizes a genetic algorithm (GA) implemented using the MATLAB GA toolbox.\nData Acquisition and Preprocessing:\nThe methodology leverages a dataset containing impeller geometries and their corresponding performance data, including flow rate (Q), head (H), diameter (D), and pump power (P). While the dataset doesn’t include a direct measurement of efficiency (\\(\\eta\\)), it can be calculated using the provided information and the following equation:\n\\[\nP = \\frac{Q*\\rho*g*H}{\\eta}\n\\]\nwhere:\n\n\\(\\eta\\) is the pump efficiency (-)\n\\(\\rho\\) is the fluid density \\((kg/m^3)\\)\n\\(g\\) is the acceleration due to gravity \\((m/s^2)\\)\n\nThis data can be obtained from various sources, such as experimental measurements, computational fluid dynamics (CFD) simulations, or a combination of both. Our code (refer to the provided MATLAB code for specific details)1 performs the following data preprocessing steps:\n\nData Cleaning: The data is inspected for missing values or outliers. Missing values can be addressed through techniques like imputation or deletion, while outliers may require further investigation or removal if they significantly impact the training process.\nNormalization: The data is normalized to a specific range (often -1 to 1 or 0 to 1) using techniques like mapminmax in MATLAB. This helps ensure the gradients calculated during backpropagation have a more consistent magnitude, facilitating faster convergence and avoiding local minima during training.\n\nNeural Network Architecture:\nThe proposed NN architecture utilizes a supervised learning approach for regression. The specific details in the code will determine the exact structure, but a typical configuration might involve:\n\nInput Layer: This layer consists of two neurons, corresponding to the input features: flow rate (Q) and head (H).\nHidden Layer(s): One or more hidden layers are employed to learn complex relationships between the input and output data. The GA will optimize the number of neurons in the hidden layer(s) based on performance.\nOutput Layer: The output layer contains a single neuron responsible for predicting the impeller diameter (D).\n\nThe activation functions used in each layer will also be optimized by the GA. The code specifically uses two common choices for activation functions in regression tasks:\n\nSigmoid (Logistic Function): This function takes the form f(x) = 1 / (1 + exp(-x)). It squashes the input values between 0 and 1, introducing non-linearity into the network. Mathematically, the sigmoid function can be expressed as: \\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nTanh (Hyperbolic Tangent Function): This function takes the form f(x) = (tanh(x)) = (e^x - e^{-x}) / (e^x + e^{-x}). It squashes the input values between -1 and 1, providing a wider range of output compared to the sigmoid function. Mathematically, the tanh function can be expressed as: \\[\nf(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\n\nBoth sigmoid and tansig (which is mathematically equivalent to tanh) functions introduce non-linearity into the network, allowing it to model complex relationships between the input and output data. The GA will evaluate the performance of the NN using different activation functions (sigmoid and tansig) and select the one that leads to the minimal MSE between predicted and actual data.\nThe tansig function used in your code is defined as: \\[\nf(x) = tansig(x) = \\frac{2}{1 + exp(-2x)} - 1\n\\]\nGenetic Algorithm (GA) for Hyperparameter Optimization:\nThe GA serves as the core optimization engine, searching for the optimal combination of NN hyperparameters that minimizes the mean squared error (MSE) between the network’s predicted diameter (D) and the actual values in the training data. and here is some glimpses on how in general it works:\n\nPopulation Initialization: The GA starts with a population of individuals (candidate hyperparameter sets). Each individual represents a unique configuration for the NN, including hidden layer size, training function, activation function, and maximum epochs.\nFitness Evaluation: Each individual in the population undergoes evaluation. The code trains an NN using the specific hyperparameters encoded in the individual’s chromosome. The resulting NN’s performance is then measured by calculating the MSE between its predicted diameter (D) and the actual values on a validation dataset (a portion of the original data set aside for evaluation).\nSelection: Based on the fitness values (MSE), the GA selects a subset of individuals with superior performance (low MSE) to become parents for the next generation.\nCrossover: Selected parent individuals undergo crossover, where portions of their genetic material (hyperparameter configurations) are exchanged to create new offspring (candidate solutions).\nMutation: A small probability of mutation is introduced to introduce random variations in the offspring’s chromosomes. This helps maintain genetic diversity and explore new regions of the search space.\nReplacement and Termination: The new generation of offspring replaces a portion of the lower-performing individuals in the population. The GA iterates through these steps until a stopping criterion is met, such as reaching a maximum number of generations or achieving a desired level of convergence (minimum MSE).\n\nThe code (refer to the MATLAB code for specific details) will implement the specific selection, crossover, and mutation operators used in the GA and all of this is handled by ga the MATLAB toolbox.\nTraining and Validation:\nThe final, selected hyperparameter configuration from the GA is used to train a new NN model. This model is trained on a separate training dataset, excluding the data used for validation during the GA optimization process. The trained model is then evaluated on a hold-out test dataset to assess its generalizability and prediction accuracy on unseen data.\nPerformance Evaluation:\nThe performance of the trained NN model is evaluated based on various metrics, including:\n\nMean Squared Error (MSE): This metric measures the average squared difference between the predicted impeller diameters (D) and the actual values in the test dataset. A lower MSE indicates better prediction accuracy.\nCoefficient of Determination (R-squared): This metric indicates the proportion of the variance in the actual diameter data explained by the NN model’s predictions. A value closer to 1 signifies a stronger correlation between the predicted and actual values.\nVisualization Techniques: Visualizations such as scatter plots comparing predicted vs. actual data."
  },
  {
    "objectID": "data/writings/posts/trimming_15-4-2024/index.html#footnotes",
    "href": "data/writings/posts/trimming_15-4-2024/index.html#footnotes",
    "title": "Beyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ngithub link to our code:https://github.com/MohammedTwheed/trimming-code ↩︎\nwe will find the dataset we used at: https://github.com/MohammedTwheed/trimming-code/tree/main/training-data↩︎"
  },
  {
    "objectID": "data/writings/index.html",
    "href": "data/writings/index.html",
    "title": "AI Applications in Pumps",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExploartion_17 : Tree-Based Genetic Programming for Polynomial Regression\n\n\nthis is for matlab_translation_symbolic_regression_18.m file implementation\n\n\n\ngenetic-algorithms\n\n\nsymbolic-regression\n\n\nMATLAB\n\n\n\nThis document is intended to be a simpler alternative for translating symbolic regression code from Python to MATLAB. It focuses on a more straightforward approach, as translating the Node and Tree classes from the Python code on GitHub (by dyckia, titled “Genetic-Programming-Polynomial-Regression”) proved challenging.\na complete example working matlab file matlab_translation_symbolic_regression_18.m will be sent with this document via whatsapp.\nlater i will upload it to github with proper permenant link.\nhere is link the translated file : - matlab_translation_symbolic_regression_18.m\nhere is link to the training data used : - A4_trainingSamples.txt \n\n\n\n\n\nJun 23, 2024\n\n\nMohammed Tawheed Khater\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond Trial and Error: A Machine Learning Approach to Optimal Centrifugal Pump Impeller Trimming\n\n\nA Genetic Algorithm Hyperparameter Optimization Approach\n\n\nThis paper proposes a genetic algorithm (GA)-based methodology to optimize hyperparameters for artificial neural networks (NNs) applied to pump impeller trimming. Traditionally, impeller trimming relies on expert knowledge and empirical methods, leading to time-consuming and potentially suboptimal outcomes. This work introduces a data-driven approach using NNs trained to predict the impeller diameter (D) and pump power (P), which can be calculated from flow rate (Q), density (\\(\\rho\\)), head (H), and efficiency (\\(\\eta\\)) using the equation P = (Q * \\(\\rho\\) * g * H) / \\(\\eta\\). based on the desired operating point (flow rate (Q) and head (H)). A GA is employed to identify the optimal NN hyperparameters, including hidden layer size, training function, activation function, and maximum epochs. The goal is to minimize the mean squared error (MSE) between the network’s predictions and the actual performance data. The paper details the implementation of the GA optimization process and discusses the key components and their significance in achieving optimal impeller trimming through NN predictions. \n\n\n\n\n\nApr 15, 2024\n\n\nMohamed Farid Khalil, Mohammed Twheed Khater, Seif Ibrahim Hassan\n\n\n\n\n\n\nNo matching items"
  }
]