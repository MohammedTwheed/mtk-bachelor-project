---
title: "Application of Artificial Intelligence on the Centrifugal Pump Operation"
author: "Mohammed Twheed Khater"
date: "2024-06-24"
cache: false
listing:
  - id: gallery
    template: gallery.ejs
    contents: gallery.yml
---

# Chapter 1: Prediction of the Effect of Impeller Trimming on Centrifugal Pump Performance Using AI

## Introduction

Centrifugal pumps are pivotal components in various industrial applications, from water supply systems to chemical processing. Optimizing their performance is crucial for enhancing energy efficiency and reducing operational costs. One key optimization technique is impeller trimming, which involves reducing the diameter of a pump impeller to align the pump's performance more closely with the system requirements. This chapter explores the concept of impeller trimming, its significance, traditional methods, and the advantages of employing Artificial Intelligence (AI) for performance prediction.

## The Concept of Impeller Trimming

### What is Trimming?

Impeller trimming is the process of mechanically reducing the diameter of the pump impeller. This adjustment directly influences the pumpâ€™s head and flow rate, thereby modifying its performance characteristics. Trimming is performed to ensure that the pump operates within the desired performance range, avoiding over-delivery or under-delivery of fluid.

### Why Trimming?

### The Need for Trimming

Trimming the impeller allows for the customization of pump performance to meet specific operational requirements. This customization is particularly necessary when:
- The available pump sizes do not perfectly match the required system specifications.
- System demands change over time, necessitating adjustments to maintain optimal efficiency.
- Reducing the operational costs by minimizing energy wastage.


#### Energy Consumption

Centrifugal pumps are often responsible for a significant portion of the energy consumption in industrial settings. Trimming the impeller to match the exact system requirements can greatly reduce the energy consumption of the pump. By operating more efficiently, the pump uses less power, leading to substantial energy savings *where for each 1 kw in pump delivery corresponds to 6 kwhr in power station*.

#### Market Availability

The pumps available in the market may not always fit specific system requirements precisely. Typically, pumps are designed for a range of operations and may be larger or smaller than needed for a particular application. Impeller trimming allows for customizing the pump's performance to meet these specific needs, ensuring that the pump operates at optimal efficiency.

## Energy Savings and Environmental Impact

Trimming the impeller is not only beneficial for energy savings but also contributes to environmental sustainability. Reduced energy consumption leads to lower greenhouse gas emissions. For every kilowatt-hour (kWh) saved by the pump, the reduction in power station output significantly decreases pollution. This correlation is particularly stark, with each kilowatt saved at the pump corresponding to approximately six kilowatts saved at the power station.

## Traditional Methods of Impeller Trimming

### Scaling Methods

Traditional methods for impeller trimming typically involve scaling laws and empirical correlations derived from extensive testing and experience. These methods include:



#### Constant-Area Scaling

Constant-area scaling assumes that the trimmed impeller maintains a constant area, ensuring proportional changes in flow and head. This method involves adjusting the impeller diameter while maintaining the proportional relationship between the flow rate and head.

$$
\text{constant-area scaling:} \quad \frac{Q'}{Q} = \frac{D_2'}{D_2} \frac{H'}{H} = \left( \frac{D_2'}{D_2} \right)^2
$$

## Artificial Neural Networks for Impeller Trimming

Artificial Neural Networks (ANNs) offer a robust alternative to traditional methods by leveraging large datasets to predict pump performance accurately. Unlike empirical methods, ANNs can model complex, non-linear relationships between variables, providing more precise predictions.

### Advantages of Neural Networks

- **Accuracy**: ANNs can learn from vast amounts of data, capturing intricate patterns and relationships that traditional methods might miss.
- **Efficiency**: Once trained, ANNs can quickly predict performance outcomes for different impeller diameters, saving time and resources.
- **Adaptability**: Neural networks can be updated with new data, continuously improving their predictive capabilities.

### Neural Network Architecture

The architecture of the neural network plays a crucial role in its performance. Key components include:
- **Input Layer**: Represents the features or variables used for prediction, such as flow rate and diameter.
- **Hidden Layers**: Intermediate layers that process the inputs through weighted connections. The number of hidden layers and neurons per layer is optimized using hyperparameter tuning.
- **Output Layer**: Provides the predicted performance metrics, such as head and power.

### Training the Neural Network

The training process involves adjusting the weights of the network to minimize the error between predicted and actual values. This is achieved through backpropagation and optimization algorithms.

### Hyperparameters and Unlearnable Parameters

Hyperparameters are settings that you adjust before training your neural network. These parameters influence the training process and the structure of the network. They are not learned from the data but are set by the user. In our code, we have chosen to optimize several hyperparameters, including:

- **Number of neurons in the hidden layers**: This determines the capacity of the neural network to learn from the data. More neurons can capture more complex patterns but may also lead to overfitting if not managed properly.
- **Training method**: This is specified by the choice of optimizer. In our code, we use the Levenberg-Marquardt (`trainlm` in MATLAB) optimizer for its efficiency in training feedforward networks.
- **Number of epochs**: Epochs refer to the number of complete passes through the training dataset. Our code optimizes the number of epochs to ensure the model is well-trained without overfitting.
- **Activation functions**: These functions define the output of each neuron. We experiment with different activation functions like `tansig` and `logsig` to find the best fit for our model.

## Genetic Algorithm for Hyperparameter Optimization

Genetic algorithms (GAs) are a class of optimization techniques inspired by the process of natural selection. They are particularly useful for optimizing complex problems with large search spaces, such as neural network hyperparameter tuning.

### How Genetic Algorithms Work

1. **Initialization**: A population of potential solutions (individuals) is generated. Each individual represents a set of hyperparameters.
2. **Selection**: Individuals are selected based on their fitness, which is typically a function of how well they perform on a given task (e.g., predicting pump performance).
3. **Crossover**: Pairs of individuals are combined to produce offspring. This process involves swapping parts of their hyperparameter sets to create new solutions.
4. **Mutation**: Some offspring undergo random changes to introduce diversity into the population.
5. **Evaluation**: The fitness of the new generation is evaluated, and the best individuals are selected for the next iteration.
6. **Termination**: The algorithm repeats the selection, crossover, mutation, and evaluation steps until a stopping criterion is met (e.g., a predefined number of generations or a satisfactory fitness level).

### Using Genetic Algorithms in Our Code

In our code, we use a genetic algorithm to optimize the hyperparameters of our neural network. The key steps involved are:

1. **Define the Search Space**: We specify the range for each hyperparameter, such as the number of neurons in hidden layers, the number of epochs, and the indices for the training and activation functions.
2. **Set Genetic Algorithm Options**: We configure the genetic algorithm with options like population size, maximum number of generations, crossover fraction, and fitness limit.
3. **Evaluate Hyperparameters**: A fitness function evaluates the performance of each set of hyperparameters. It trains the neural network and computes the mean squared error (MSE) across training, validation, and testing datasets.
4. **Optimize**: The genetic algorithm iteratively searches for the optimal hyperparameters by generating new populations, evaluating their fitness, and selecting the best-performing sets.

Here is an outline of the relevant code:

```matlab
% Define bounds for the genetic algorithm optimization
lower_bounds = [2,  13,    13, 1, 1];
upper_bounds = [2,  300,    300, 2, 1];

% Genetic algorithm options
gaOptions = optimoptions('ga', ...
    'PopulationSize', 17, ...
    'MaxGenerations', 13, ...
    'CrossoverFraction', 0.8, ...
    'ConstraintTolerance', 0.000991, ...
    'FitnessLimit', 0.000991, ...
    'EliteCount', 2, ...
    'Display', 'iter', ...
    'UseParallel', true);

% Optimization using genetic algorithm
[optimalHyperParams, finalMSE] = ga(@(hyperParams) evaluateHyperparameters(hyperParams, x, t, randomSeed), ...
    length(lower_bounds), [], [], [], [], lower_bounds, upper_bounds, [], gaOptions);
```

In this section of the code, we set up the bounds for the hyperparameters and configure the genetic algorithm options. The `evaluateHyperparameters` function is called by the genetic algorithm to assess the performance of each set of hyperparameters, guiding the search towards the optimal solution.

The combination of neural networks and genetic algorithms provides a powerful approach for predicting pump performance with high accuracy and efficiency, leveraging advanced optimization techniques to fine-tune the model.
## Genetic Algorithm for Hyperparameter Optimization

A genetic algorithm (GA) is used to optimize the hyperparameters of the neural network. GA is a search heuristic that mimics the process of natural selection, making it effective for exploring large and complex search spaces.

### Key Steps in Genetic Algorithm

1. **Initialization**: Creating an initial population of potential solutions with random hyperparameters.
2. **Selection**: Evaluating the fitness of each individual in the population based on the mean squared error (MSE) of the neural network's predictions.
3. **Crossover**: Combining pairs of individuals to produce offspring with mixed characteristics, promoting the inheritance of good traits.
4. **Mutation**: Introducing random changes to some individuals to maintain genetic diversity and explore new solutions.
5. **Evaluation**: Assessing the performance of the new population and iterating through the selection, crossover, and mutation steps until convergence or a predefined number of generations is reached.

## Implementation in MATLAB

The implementation of AI for impeller trimming was carried out using MATLAB. The scripts `main_04.m` and `QHforDiameters.m` are critical components of this implementation, leveraging optimized neural network architectures to predict pump performance based on different impeller diameters.

### Script: main_04.m

The `main_04.m` script incorporates the following key steps:

1. **Data Loading**: Loading datasets containing flow rate, head, diameter, and power metrics.
2. **Network Training**: Training neural networks with optimized architectures to predict head and power based on flow rate and diameter.
3. **Performance Evaluation**: Evaluating the trained networks on various performance metrics to ensure accuracy and reliability.
4. **Visualization**: Generating 3D plots to visualize the relationship between flow rate, head, diameter, and power, showcasing the neural network predictions versus actual data.

#### Key Functions and Their Roles

- **train_nn**: This function trains the neural network using the provided data, returning the trained model and performance metrics.
- **trim_diameters**: This function determines the optimal trimmed diameter based on the provided pump data and performance criteria.
- **processDataAndVisualize**: This function processes the data and generates visualizations to compare neural network predictions with actual data points.

#### Sample Code Snippet from `main_04.m`

The following MATLAB code snippet from `main_04.m` demonstrates how to load data, train neural networks, and visualize the results:

```matlab
clear; clc; clf; close all;

% Load data
load('filtered_QHD_table.mat');
load('filtered_QDP_table.mat');
load('deleted_QHD_table.mat');
load('deleted_QDP_table.mat');

% Extract data
QH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';
D = [filtered_QHD_table.Diameter_mm]';
QD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';
P = [filtered_QDP_table.Power_kW]';

% Train on full dataset
[trainedNetQHD, ~, ~, ~, ~] = train_nn([2, 16], 191, 'trainlm', QH, D, 4837);
[trainedNetQDP, ~, ~, ~, ~] = train_nn([2, 7, 29, 17], 191, 'trainlm', QD, P, 4837);

% Visualization
processDataAndVisualize(QH', D', QD', P', trainedNetQHD, trainedNetQDP, 'figures');
```

### Script: QHforDiameters.m

The `QHforDiameters.m` script focuses on optimizing neural network hyperparameters for better performance prediction. It uses a genetic algorithm to find the optimal neural network architecture, ensuring accurate predictions for different impeller diameters.

#### Key Steps in `QHforDiameters.m`

1. **Initialization**: Loading data and initializing variables.
2. **Hyperparameter Optimization**: Using a genetic algorithm to find the optimal neural network architecture.
3. **Performance Evaluation**: Assessing the neural network's performance on the training and test datasets.
4. **Visualization**: Plotting the predicted performance curves for different impeller diameters.

#### Sample Code Snippet from `QHforDiameters.m`

The following MATLAB code snippet from `QHforDiameters.m` illustrates the process of optimizing neural network hyperparameters and visualizing the results:

```matlab
clear; clc; clf;
load('filtered_QHD_table.mat');
load('filtered_QDP_table.mat');
load('deleted_QHD_table.mat');
load('deleted_QDP_table.mat');

QH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';
D  = [filtered_QHD_table.Diameter_mm]';

QH_beps=[deleted_QHD_table.FlowRate_m3h,deleted_QHD_table.Head_m]';
D_beps

=[deleted_QHD_table.Diameter_mm]';

QD = [filtered_QDP_table.FlowRate_m3h,filtered_QDP_table.Diameter_mm]';
P = [filtered_QDP_table.Power_kW]';

QD_beps=[deleted_QDP_table.FlowRate_m3h,deleted_QDP_table.Diameter_mm]';
P_beps=[deleted_QDP_table.Power_kW]';

% User-specified random seed (optional)
userSeed = 4826;

% Define a threshold for MSE to exit the loop early
mseThreshold = 0.000199;

% Initialize result matrix
result = [];

% Find all distinct diameters in D
distinctDiameters = unique(D);

% Weights for combining MSEs
weightDiameter = 0.5;
weightBeps = 0.5;

for dIdx = 1:length(distinctDiameters)
    % Current diameter to remove
    diameterToRemove = distinctDiameters(dIdx);
    
    % Remove the current diameter from QH and D
    idxToKeep = D ~= diameterToRemove;
    QH_filtered = QH(:, idxToKeep);
    D_filtered = D(idxToKeep);
    
    % Calculate the number of epochs based on dataset size
    maxEpochs = 1000 + floor(size(QH_filtered, 2) / 10);
    
    % Determine the number of hidden layers and neurons to search
    hiddenLayers = [1:10];
    neuronsPerLayer = [1:30];
    
    % Initialize the best MSE and corresponding architecture
    bestMSE = Inf;
    bestArch = [];
    
    % Random seed for reproducibility
```

## Results

### QHD results table

```{python}
import pandas as pd

# Load the QHD results table
qhd_results = pd.read_csv('QHD_results.csv')
# Rename columns to ensure headers are displayed correctly
qhd_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']
qhd_results
```

### QDP results tabel

```{python}
# Load the QDP results table
qdp_results = pd.read_csv('QDP_results.csv')
qdp_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']
qdp_results
```

### QDH results tabel

```{python}
# Load the QDH results table
qdh_results = pd.read_csv('QDH_results.csv')
qdh_results.columns = ['DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs']
qdh_results
```

### errors and reductions

here we compare the best neural network we have with the traditional *constant area scaling methode form [weme paper need citation]* where 

```matlab
% Loop through each column in QH_beps
for i = 1:5
    d_real = D_beps(1, i); % Extracting d_real from D_beps

    % Calculate d using constant_area_scaling
    d_trimmed_cas_260 = constant_area_scaling(QH_beps(1, i), QH_beps(2, i), pump_data(5).Q, pump_data(5).H, pump_data(5).Diameter, 4);
    percent_errors_cas_260(i) = abs((d_trimmed_cas_260 - d_real) / d_real) * 100;

    % Calculate d using trim_diameters
    d_trimmed_cas_nearest = trim_diameters(QH_beps(:, i), 'filtered_QHD_table.mat');
    percent_errors_cas_nearest(i) = abs((d_trimmed_cas_nearest - d_real) / d_real) * 100;

    % Calculate d using trainedNetQHD
    d_trimmed_nn = bestNetQHD.net(QH_beps(:, i));
    percent_errors_nn(i) = abs((d_trimmed_nn - d_real) / d_real) * 100;

    % Calculate percent reduction in diameter
    percent_reductions(i) = abs((d_real - d_trimmed_nn) / d_real) * 100;
end

```

as we see here we do so in two different variants where in `percent_errors_cas_260` we feed to `constant_area_scaling` the last diameter the  $ 260\:mm $ diameter as the base diameter and for all the test points which are the best effecincy points they will trimm form it and this gives greater error as you see below.

while `trim_diameters` the function will find the nearst curve from the given 5 curves to trim againist it and this will reduce the error so much but it still higher that the nn by order of magnitude.



```{python}
erros_reductions = pd.read_csv('errors_and_reductions.csv')
erros_reductions .columns = ['Index', 'Percent_Error_CAS_260', 'Percent_Error_CAS_Nearest', 'Percent_Error_NN', 'Percent_Reduction']
erros_reductions 
```


### final stats

```{python}
fstats = pd.read_csv('final_statistics.csv')
fstats.columns = ['MAE_Trim_Diameters', 'MAE_TrainedNetQHD', 'Count_Better_TrainedNetQHD', 'Count_Better_Trim_Diameters']
fstats
```

# Figures


::: {#gallery .column-page}
:::


## Conclusion

The use of Artificial Intelligence, particularly neural networks and genetic algorithms, provides a powerful tool for predicting the effects of impeller trimming on centrifugal pump performance. This approach offers significant advantages in terms of accuracy, efficiency, and adaptability, making it a superior alternative to traditional methods. By optimizing pump performance, we can achieve substantial energy savings and reduce environmental impact, contributing to a more sustainable industrial practice.



---


# QHforDiameters.m code docs

this file is the pre-step to our final file main_04.m


This MATLAB script `QHforDiameters.m` demonstrates an advanced approach to optimizing pump impeller diameters using neural networks. By systematically training

 and refining the network, we achieve a model capable of accurately predicting the optimal diameter, thereby improving pump efficiency. This methodology showcases the potential of AI in engineering applications, blending data-driven insights with practical engineering challenges.

## Code Breakdown

### Initial Setup

The script begins by clearing the workspace and loading several data files:

```matlab
clear; clc; clf;
load('filtered_QHD_table.mat')
load('filtered_QDP_table.mat')
load('deleted_QHD_table.mat')
load('deleted_QDP_table.mat')
```

- `filtered_QHD_table.mat` and `filtered_QDP_table.mat`: Contain the filtered data for flow rate (`Q`), head (`H`), and power (`P`) against diameters (`D`).
- `deleted_QHD_table.mat` and `deleted_QDP_table.mat`: Contain the data points excluded from the filtered tables.

### Data Preparation

The loaded data is then organized into matrices for further processing:

```matlab
QH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';
D  = [filtered_QHD_table.Diameter_mm]';

QH_beps = [deleted_QHD_table.FlowRate_m3h, deleted_QHD_table.Head_m]';
D_beps = [deleted_QHD_table.Diameter_mm]';

QD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';
P = [filtered_QDP_table.Power_kW]';

QD_beps = [deleted_QDP_table.FlowRate_m3h, deleted_QDP_table.Diameter_mm]';
P_beps = [deleted_QDP_table.Power_kW]';
```

Here, we arrange the data into variables for flow rate and head (`QH`), diameters (`D`), and power (`P`).

### Hyperparameter Optimization Setup

We define several key parameters for the optimization process:

```matlab
userSeed = 4826;
mseThreshold = 0.000199;
result = [];
distinctDiameters = unique(D);

weightDiameter = 0.5;
weightBeps = 0.5;
```

- `userSeed`: A seed for random number generation to ensure reproducibility.
- `mseThreshold`: The mean squared error threshold for early stopping.
- `distinctDiameters`: A unique set of diameters for which we'll optimize our neural network.

### Optimization Loop

The core of the script involves iterating over each distinct diameter, removing it from the dataset, and training a neural network to predict the head (`H`) given the flow rate (`Q`) and the diameter (`D`):

```matlab
for dIdx = 1:length(distinctDiameters)
    diameterToRemove = distinctDiameters(dIdx);
    indicesToRemove = find(D == diameterToRemove);
    removedQH = QH(:, indicesToRemove);
    removedD = D(indicesToRemove);
    QH_temp = QH;
    D_temp = D;
    QH_temp(:, indicesToRemove) = [];
    D_temp(:, indicesToRemove) = [];
    
    Qa = QH_temp(1,:);
    Ha = QH_temp(2,:);
    Q = QH_temp(1,:);
    H = QH_temp(2,:);

    lower_bounds = [2, 13, 13, 1, 1];
    upper_bounds = [2, 300, 300, 2, 1];
    prevCombinedMSE = inf;

    for i = 1:20
        [optimalHyperParamsH, finalMSEH, randomSeedH, bestTrainedNetH, error] = ...
            optimizeNNForTrimmingPumpImpeller([QH_temp(1,:); D_temp], QH_temp(2,:), userSeed+i, lower_bounds, upper_bounds);

        result(i, :) = [i, optimalHyperParamsH, finalMSEH, randomSeedH, error(1), error(2), error(3)];
        predictedH = bestTrainedNetH([removedQH(1, :); removedD])';
        mseDiameter = mean((removedQH(2, :)' - predictedH).^2 / sum(removedQH(2, :)));

        predictedH_beps = bestTrainedNetH([QH_beps(1,:); D_beps])';
        mseQH_beps = mean((QH_beps(2,:)' - predictedH_beps).^2 / sum(QH_beps(2,:)));

        fprintf('Diameter %d, Iteration %d, MSE_Dia: %.6f,  MSE_beps: %.6f \n', diameterToRemove, i, mseDiameter, mseQH_beps);

        combinedMSE = weightDiameter * mseDiameter + weightBeps * mseQH_beps;
        deltaMSE = prevCombinedMSE - combinedMSE;
        
        if deltaMSE > 0.01
            adjustment = [0, 5, 15, 0, 0];
        elseif deltaMSE > 0.001
            adjustment = [0, 2, 10, 0, 0];
        else
            adjustment = [0, 1, 5, 0, 0];
        end
        
        lower_bounds = max(lower_bounds, [2, optimalHyperParamsH(2), optimalHyperParamsH(3), 1, 1] - adjustment);
        upper_bounds = min(upper_bounds, [2, optimalHyperParamsH(2), optimalHyperParamsH(3), 2, 1] + adjustment);
        prevCombinedMSE = combinedMSE;

        if (mseDiameter < mseThreshold) && (error(3) < 0.0199) && (mseQH_beps < mseThreshold)
            fprintf('MSE for diameter %d is below the threshold. Exiting loop.\n', diameterToRemove);
            break;
        end
    end
end
```

### Plotting Results

The script generates plots to visualize the performance of the neural network:

```matlab
for diameterIndex = 1:length(desiredDiameters)
    desiredDiameter = desiredDiameters(diameterIndex);
    Dt = repmat(desiredDiameter, length(Qt), 1);
    filteredQH = bestTrainedNetH([Qt; Dt'])';

    legendLabel = strcat('Diameter: ', num2str(desiredDiameter), 'mm');
    plot(Qt, filteredQH, 'DisplayName', legendLabel);
    text(Qt(end), filteredQH(end), sprintf('%dmm', desiredDiameter), 'FontSize', 8, 'Color', 'black', 'BackgroundColor', 'white');
end

scatter(Qa', Ha', 'b', 'filled', 'DisplayName', 'Reference Points');
xlabel('Q (m^3/h)');
ylabel('H (m)');
title(['(Q, H) slices with Diameters, Removed Diameter: ' num2str(diameterToRemove) 'mm']);
legend;
hold off;
```

### Saving Results

The results and trained networks are saved for further analysis:

```matlab
filename = sprintf('../loop_05/01/nn_diameter-%d_iteration_%d_%d-%d-%d-%d-%d_mseDia-%d_test-%d.png', diameterToRemove, i, ...
    optimalHyperParamsH(1), optimalHyperParamsH(2), optimalHyperParamsH(3), ...
    optimalHyperParamsH(4), optimalHyperParamsH(5), mseDiameter, error(3));
saveas(gcf, filename);

filename = sprintf('../loop_05/01/nn_diameter-%d_iteration_%d_%d-%d-%d-%d-%d_mseDia-%d_test-%d.mat', diameterToRemove, i, ...
    optimalHyperParamsH(1), optimalHyperParamsH(2), optimalHyperParamsH(3), ...
    optimalHyperParamsH(4), optimalHyperParamsH(5), mseDiameter, error(3));
save(filename, 'bestTrainedNetH');
```

### Writing Results to CSV

Finally, the results of the optimization are written to a CSV file for documentation:

```matlab
writematrix([["Iteration", "Hidden Layer 1 Size", "Hidden Layer 2 Size", "Max Epochs", ...
    "Training Function", "Activation Function", "Final MSE", ...
    "Random Seed", "Training Error", "Validation Error", "Test Error"]; result], './01/results_loop.csv');

disp('./01/Results saved to results_loop.csv');
```

### Supporting Functions

Several supporting functions handle data loading, neural network optimization, and visualization:

```matlab
function [QH, D, QD, P] = loadData(dataPath)
% Function to load data from specified path
% ...
end

function [optimalHyperParams, finalMSE, randomSeed, bestTrainedNet, nnPerfVect] = optimizeNNForTrimmingPumpImpeller(x, t, userSeed, lowerBounds, upperBounds)
% Function to optimize neural network hyperparameters using genetic algorithm
% ...
end

function processDataAndVisualize(QH, D, QD, P, bestTrainedNetD, bestTrainedNetP, saveFigures)
% Function to process data and generate visualizations
% ...
end
```



---


# main_04.m

This MATLAB script  `main_04.m` exemplifies the application of neural networks in optimizing pump performance by predicting outcomes such as flow rate, head, and power. By training models on different subsets of data, it ensures robustness and generalization, leading to improved design and operational efficiency in pump systems. This project can be extended further by incorporating more advanced AI techniques and real-time data for continuous optimization.



The code performs the following key steps:
1. Load and preprocess data.
2. Train neural networks on the data.
3. Evaluate the trained models.
4. Use the trained models to predict and visualize performance under various conditions.

### Details

#### Initialization and Data Loading

```matlab
clear; clc; clf; close all;

% Load data
load('filtered_QHD_table.mat');
load('filtered_QDP_table.mat');
load('deleted_QHD_table.mat');
load('deleted_QDP_table.mat');
```
- `clear; clc; clf; close all;` clears the workspace, command window, figure window, and closes any open figures.
- The `load` commands import datasets from `.mat` files into the workspace. These datasets contain performance data of pumps under different conditions.

#### Data Extraction

```matlab
% Extract data
QH = [filtered_QHD_table.FlowRate_m3h, filtered_QHD_table.Head_m]';
D = [filtered_QHD_table.Diameter_mm]';
QH_beps = [deleted_QHD_table.FlowRate_m3h, deleted_QHD_table.Head_m]';
D_beps = [deleted_QHD_table.Diameter_mm]';
QD = [filtered_QDP_table.FlowRate_m3h, filtered_QDP_table.Diameter_mm]';
P = [filtered_QDP_table.Power_kW]';
QD_beps = [deleted_QDP_table.FlowRate_m3h, deleted_QDP_table.Diameter_mm]';
P_beps = [deleted_QDP_table.Power_kW]';
```
- The data is extracted into separate variables for easier handling.
  - `QH` contains flow rate and head data.
  - `D` contains diameter data.
  - `QH_beps` and `D_beps` contain best efficiency point (BEP) data.
  - `QD` contains flow rate and diameter data related to power.
  - `P` contains power data.

#### Creating Output Directories

```matlab
% Create output directories
output_dir = 'out_data';
figures_dir = fullfile(output_dir, 'figures');
if ~exist(output_dir, 'dir')
    mkdir(output_dir);
end
if ~exist(figures_dir, 'dir')
    mkdir(figures_dir);
end
```
- The code creates directories to save the output data and figures if they don't already exist.

#### Neural Network Training Parameters

```matlab
% Hyperparameters based on latest optimization with GA
randomSeed = 4837;
nn_QHD_size_matrix = [2, 16];
nn_QDH_size_matrix = [2, 16];
nn_QDP_size_matrix = [2, 7, 29, 17];
maxEpochs = 191;
trainFcn = 'trainlm';
```
- The hyperparameters for the neural networks are defined, including the random seed for reproducibility, network architecture (size of hidden layers), number of epochs for training, and the training function (`trainlm` - Levenberg-Marquardt).

#### Training Neural Networks on Full Dataset

```matlab
[trainedNetQHD, avgMSEsQHD, trainPerformanceQHD, valPerformanceQHD, testPerformanceQHD] = train_nn(nn_QHD_size_matrix, maxEpochs, trainFcn, QH, D, randomSeed);
[trainedNetQDH, avgMSEsQDH, trainPerformanceQDH, valPerformanceQDH, testPerformanceQDH] = train_nn(nn_QDH_size_matrix, maxEpochs, trainFcn, [QH(1,:); D], QH(2,:), randomSeed);
[trainedNetQDP, avgMSEsQDP, trainPerformanceQDP, valPerformanceQDP, testPerformanceQDP] = train_nn(nn_QDP_size_matrix, maxEpochs, trainFcn, QD, P, randomSeed);
```
- Neural networks are trained on the full dataset for each model (QHD, QDH, and QDP) using the specified parameters.

#### Initializing Results Tables and Logs

```matlab
QHD_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});
QDP_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});
QDH_results = array2table(NaN(1, 7), 'VariableNames', {'DiameterRemoved', 'AvgMSE', 'TrainPerformance', 'ValPerformance', 'TestPerformance', 'MSEDeletedDiameter', 'MSEBEPs'});

logs = {};  % Initialize logs
```
- Results tables and logs are initialized to store the performance metrics and log messages during the training process.

#### Weights for Error Calculation

```matlab
weights = struct('train', 0.05, 'val', 0.05, 'test', 0.35, 'deleted_diameter', 0.45, 'beps', 0.1);
```
- Weights are defined to calculate a weighted score for model performance, giving different importance to training, validation, test errors, and errors on specific data points.

#### Best Neural Network Initialization

```matlab
bestNetQHD = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);
bestNetQDP = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);
bestNetQDH = struct('net', [], 'diameter', [], 'score', Inf, 'trainPerformance', [], 'valPerformance', [], 'testPerformance', []);
```
- Structures are initialized to store the best neural networks for each model based on their performance.

#### Function to Compute Weighted Score

```matlab
compute_score = @(trainPerf, valPerf, testPerf, mseDeleted, mseBEPS, weights) ...
    weights.train * trainPerf + weights.val * valPerf + weights.test * testPerf + weights.deleted_diameter * mseDeleted + weights.beps * mseBEPS;
```
- A function is defined to compute the weighted score for model performance based on the defined weights.

### Training with Different Diameters Hidden (QHD and QDH)

```matlab
distinctDiametersQHD = unique(D);
for dIdx = 1:length(distinctDiametersQHD)
    diameterToRemove = distinctDiametersQHD(dIdx);
    indicesToRemove = find(D == diameterToRemove);
    removedQH = QH(:, indicesToRemove);
    removedD = D(indicesToRemove);
    QH_temp = QH;
    D_temp = D;
    QH_temp(:, indicesToRemove) = [];
    D_temp(:, indicesToRemove) = [];

    try
        [trainedNetQHD_temp, avgMSEsQHD_temp, trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp] = train_nn(nn_QHD_size_matrix, maxEpochs, trainFcn, QH_temp, D_temp, randomSeed);
        mse_deleted_diameter = perform(trainedNetQHD_temp, removedD, trainedNetQHD_temp(removedQH));
        mse_beps = perform(trainedNetQHD_temp, D_beps, trainedNetQHD_temp(QH_beps));
        logs{end+1} = ['Trained nn_QHD_temp on dataset without diameter ' num2str(diameterToRemove) ' successfully.'];

        score = compute_score(trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp, mse_deleted_diameter, mse_beps, weights);

        QHD_results = [QHD_results; {diameterToRemove, avgMSEsQHD_temp, trainPerformanceQHD_temp, valPerformanceQHD_temp, testPerformanceQHD_temp, mse_deleted_diameter, mse_beps}];

        if score < bestNetQHD.score
            bestNetQHD.net = trainedNetQHD_temp;
            bestNetQHD.diameter = diameterToRemove;
            bestNetQHD.score = score;
            bestNetQHD.trainPerformance = trainPerformanceQHD_temp;
            bestNetQHD.valPerformance = valPerformanceQHD_temp;
            bestNetQHD.testPerformance = testPerformanceQHD_temp;
        end

        figure;
        plot(QH(1,:), QH(2,:), 'bo', 'DisplayName', 'Original Data');
        hold on;
        plot(QH_temp(1,:), trainedNetQHD_temp([QH_temp(1,:); D_temp]), 'r*', 'DisplayName', 'Trained Net Predictions');
        plot(removedQH(1,:), removedQH(2,:), 'gx', 'DisplayName', 'Removed Diameter Data

');
        hold off;
        title(['QHD Model without Diameter ' num2str(diameterToRemove)]);
        legend;
        saveas(gcf, fullfile(figures_dir, ['QHD_wo_diameter_' num2str(diameterToRemove) '.png']));

    catch ME
        logs{end+1} = ['Error training nn_QHD_temp on dataset without diameter ' num2str(diameterToRemove) ': ' ME.message];
    end
end
```
- The above loop trains the QHD model by leaving out one diameter at a time and evaluates the performance on the remaining data. It then calculates the weighted score and updates the best model if the current one performs better.
- Similar loops are used for QDH and QDP models.

### Finalizing and Saving Results

```matlab
% Save results and best networks
save(fullfile(output_dir, 'QHD_results.mat'), 'QHD_results');
save(fullfile(output_dir, 'QDP_results.mat'), 'QDP_results');
save(fullfile(output_dir, 'QDH_results.mat'), 'QDH_results');
save(fullfile(output_dir, 'bestNetQHD.mat'), 'bestNetQHD');
save(fullfile(output_dir, 'bestNetQDP.mat'), 'bestNetQDP');
save(fullfile(output_dir, 'bestNetQDH.mat'), 'bestNetQDH');
```
- The results and the best models are saved to files for future use and analysis.




---

